{"cells":[{"cell_type":"markdown","source":["# CMU 10-405/10-605 auto-graded notebook\n\nBefore you turn this assignment in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n\nMake sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE.\""],"metadata":{}},{"cell_type":"markdown","source":["---"],"metadata":{}},{"cell_type":"markdown","source":["#CMU 10405/10605 Machine Learning with Large Datasets\n\n## Homework 2: ML Pipeline & Linear Regression"],"metadata":{}},{"cell_type":"code","source":["# Who did you collaborate with on this assignment? \n# if no one, collaborators should contain an empty string,\n# else list your collaborators below\n\ncollaborators = [\"\"]"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"collaborators","locked":false,"solution":true,"checksum":"fe60cd9540617cc2e184126ec775bb7a","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":["try:\n    collaborators\nexcept:\n    raise AssertionError(\"you did not list your collaborators, if any\")   "],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_collaborators","locked":true,"solution":false,"points":0,"checksum":"50c5e9b3db94ec533690bf66a486406a","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":["# YOU CAN MOST LIKELY IGNORE THIS CELL. This is only of use for running this notebook locally.\n\n# THIS CELL DOES NOT NEED TO BE RUN ON DATABRICKS. \n# Note that Databricks already creates a SparkContext for you, so this cell can be skipped.\n# import findspark\n# findspark.init()\n# import pyspark\n# from pyspark.sql import SQLContext\n# sc = pyspark.SparkContext(appName=\"hw\")\n# sqlContext = SQLContext(sc)\n\n# print(\"spark context started\")"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["#I. Power Plant Machine Learning Pipeline Application\nThis section is an end-to-end exercise of performing Extract-Transform-Load and Exploratory Data Analysis on a real-world dataset, and then applying several different machine learning algorithms to solve a supervised regression problem on the dataset.\n\n** This excercise covers: **\n* *Part 1: Business Understanding*\n* *Part 2: Load Your Data*\n* *Part 3: Explore Your Data*\n* *Part 4: Visualize Your Data*\n* *Part 5: Data Preparation*\n* *Part 6: Data Modeling*\n* *Part 7: Tuning and Evaluation*\n\n*Our goal is to accurately predict power output given a set of environmental readings from various sensors in a natural gas-fired power generation plant.*\n\n\n** Background **\n\nPower generation is a complex process, and understanding and predicting power output is an important element in managing a plant and its connection to the power grid. The operators of a regional power grid create predictions of power demand based on historical information and environmental factors (e.g., temperature). They then compare the predictions against available resources (e.g., coal, natural gas, nuclear, solar, wind, hydro power plants). Power generation technologies such as solar and wind are highly dependent on environmental conditions, and all generation technologies are subject to planned and unplanned maintenance.\n\nHere is an real-world example of predicted demand (on two time scales), actual demand, and available resources from the California power grid: http://www.caiso.com/Pages/TodaysOutlook.aspx\n\n![](http://content.caiso.com/outlook/SP/ems_small.gif)\n\nThe challenge for a power grid operator is how to handle a shortfall in available resources versus actual demand. There are three solutions to  a power shortfall: build more base load power plants (this process can take many years to decades of planning and construction), buy and import power from other regional power grids (this choice can be very expensive and is limited by the power transmission interconnects between grids and the excess power available from other grids), or turn on small [Peaker or Peaking Power Plants](https://en.wikipedia.org/wiki/Peaking_power_plant). Because grid operators need to respond quickly to a power shortfall to avoid a power outage, grid operators rely on a combination of the last two choices. In this exercise, we'll focus on the last choice.\n\n** The Business Problem **\n\nBecause they supply power only occasionally, the power supplied by a peaker power plant commands a much higher price per kilowatt hour than power from a power grid's base power plants. A peaker plant may operate many hours a day, or it may operate only a few hours per year, depending on the condition of the region's electrical grid. Because of the cost of building an efficient power plant, if a peaker plant is only going to be run for a short or highly variable time it does not make economic sense to make it as efficient as a base load power plant. In addition, the equipment and fuels used in base load plants are often unsuitable for use in peaker plants because the fluctuating conditions would severely strain the equipment.\n\nThe power output of a peaker power plant varies depending on environmental conditions, so the business problem is _predicting the power output of a peaker power plant as a function of the environmental conditions_ -- since this would enable the grid operator to make economic tradeoffs about the number of peaker plants to turn on (or whether to buy expensive power from another grid).\n\nGiven this business problem, we need to first perform Exploratory Data Analysis to understand the data and then translate the business problem (predicting power output as a function of envionmental conditions) into a Machine Learning task.  In this instance, the ML task is regression since the label (or target) we are trying to predict is numeric. We will use an [Apache Spark ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark-ml-package) to perform the regression.\n\nThe real-world data we are using in this notebook consists of 9,568 data points, each with 4 environmental attributes collected from a Combined Cycle Power Plant over 6 years (2006-2011), and is provided by the University of California, Irvine at [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant). You can find more details about the dataset on the UCI page, including the following background publications:\n* Pinar Tüfekci, [Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods](http://www.journals.elsevier.com/international-journal-of-electrical-power-and-energy-systems/), International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615.\n* Heysem Kaya, Pinar Tüfekci and Fikret S. Gürgen: [Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine](http://www.cmpe.boun.edu.tr/~kaya/kaya2012gasturbine.pdf), Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai).\n\n**To Do**: Read the documentation and examples for [Spark Machine Learning Pipeline](https://spark.apache.org/docs/1.6.2/ml-guide.html#main-concepts-in-pipelines)."],"metadata":{}},{"cell_type":"markdown","source":["## Part 1: Business Understanding\nThe first step in any machine learning task is to understand the business need.\n\nAs described in the overview we are trying to predict power output given a set of readings from various sensors in a gas-fired power generation plant.\n\nThe problem is a regression problem since the label (or target) we are trying to predict is numeric."],"metadata":{}},{"cell_type":"markdown","source":["## Part 2: Extract-Transform-Load (ETL) Your Data\n\nNow that we understand what we are trying to do, the first step is to load our data into a format we can query and use.  This is known as ETL or \"Extract-Transform-Load\".  We will load our file from Amazon S3.\n\nNote: Alternatively we could upload our data using \"Databricks Menu > Tables > Create Table\", assuming we had the raw files on our local computer.\n\nOur data is available on Amazon s3 at the following path:\n\n```\ndbfs:/databricks-datasets/power-plant/data\n```\n\n**To Do:** Let's start by printing a sample of the data.\n\nWe'll use the built-in Databricks functions for exploring the Databricks filesystem (DBFS)\n\nUse `display(dbutils.fs.ls(\"/databricks-datasets/power-plant/data\"))` to list the files in the directory"],"metadata":{}},{"cell_type":"code","source":["# Uncomment the following code to run it in databricks\n\ndisplay(dbutils.fs.ls(\"/databricks-datasets/power-plant/data\"))"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Next, use the `dbutils.fs.head` command to look at the first 65,536 bytes of the first file in the directory.\n\nUse `print dbutils.fs.head(\"/databricks-datasets/power-plant/data/Sheet1.tsv\")` to list the files in the directory"],"metadata":{}},{"cell_type":"code","source":["print (dbutils.fs.head(\"/databricks-datasets/power-plant/data/Sheet1.tsv\"))"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["`dbutils.fs` has its own help facility, which we can use to see the various available functions."],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.help()"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["### Exercise 2(a)\n\nNow, let's use PySpark instead to print the first 5 lines of the data.\n\n*Hint*: First create an RDD from the data by using [`sc.textFile(\"dbfs:/databricks-datasets/power-plant/data\")`](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.SparkContext.textFile) to read the data into an RDD.\n\n*Hint*: Then figure out how to use the RDD [`take()`](https://spark.apache.org/docs/1.6.2/api/python/pyspark.html#pyspark.RDD.take) method to extract the first 5 lines of the RDD and print each line."],"metadata":{}},{"cell_type":"code","source":["url = \"https://raw.githubusercontent.com/10605/data/master/hw2/Sheet\"\n\nfrom pyspark import SparkFiles\nsc.addFile(url+\"1.tsv\"); sc.addFile(url+\"2.tsv\"); sc.addFile(url+\"3.tsv\"); sc.addFile(url+\"4.tsv\"); sc.addFile(url+\"5.tsv\")\n\nrawTextRdd = sc.textFile(\"file://\" + SparkFiles.getRootDirectory(), 8)\n\n# Print the first five lines\nprint(rawTextRdd.take(5))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"print_rawTextRdd","locked":false,"solution":true,"checksum":"27840d565756979d32c89a390529568e","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["From our initial exploration of a sample of the data, we can make several observations for the ETL process:\n  - The data is a set of .tsv (Tab Seperated Values) files (i.e., each row of the data is separated using tabs)\n  - There is a header row, which is the name of the columns\n  - It looks like the type of the data in each column is consistent (i.e., each column is of type double)\n\nOur schema definition from UCI appears below:\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output.  This is the value we are trying to predict given the measurements above.\n\nWe are ready to create a DataFrame from the TSV data. Spark does not have a native method for performing this operation, however we can use [spark-csv](https://spark-packages.org/package/databricks/spark-csv), a third-party package from [SparkPackages](https://spark-packages.org/). The documentation and source code for [spark-csv](https://spark-packages.org/package/databricks/spark-csv) can be found on [GitHub](https://github.com/databricks/spark-csv). The Python API can be found [here](https://github.com/databricks/spark-csv#python-api).\n\n(**Note**: In Spark 2.0, the CSV package is built into the DataFrame API.)\n\nTo use the [spark-csv](https://spark-packages.org/package/databricks/spark-csv) package, we use the [sqlContext.read.format()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.format) method to specify the input data source format: `'com.databricks.spark.csv'`\n\nWe can provide the [spark-csv](https://spark-packages.org/package/databricks/spark-csv) package with options using the [options()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.options) method. The available options are listed in the GitHub documentation [here](https://github.com/databricks/spark-csv#features).\n\nWe will use the following three options:\n- `delimiter='\\t'` because our data is tab delimited\n- `header='true'` because our data has a header row\n- `inferschema='true'` because we believe that all of the data is double values, so the package can dynamically infer the type of each column. *Note that this will require two pass over the data.*\n\n\nThe last component of creating the DataFrame is to specify the location of the data source using the [load()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.load) method: `\"/databricks-datasets/power-plant/data\"`\n\nPutting everything together, we will use an operation of the following form:\n\n  `sqlContext.read.format().options().load()`\n\n### Exercise 2(b)\n\n**To Do:** Create a DataFrame from the data.\n\n*Hint:* Use the above template and fill in each of the methods."],"metadata":{}},{"cell_type":"code","source":["powerPlantDF = sqlContext.read.format('com.databricks.spark.csv').options(delimiter='\\t', header='true', inferschema='true').load(\"file://\" + SparkFiles.getRootDirectory())"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"createDataFrame","locked":false,"solution":true,"checksum":"44eb3c3be1a04cd4e9251fc950d64ebc","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":["# TEST\nfrom nose.tools import assert_equal, assert_true\nexpected = set([(s, 'double') for s in ('AP', 'AT', 'PE', 'RH', 'V')])\nassert_equal(expected, set(powerPlantDF.dtypes), \"Incorrect schema for powerPlantDF\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_createDataFrame","locked":true,"solution":false,"points":1,"checksum":"3e0f16daf156f72ef6f8fe8169fdb169","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":["Check the names and types of the columns using the [dtypes](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes) method."],"metadata":{}},{"cell_type":"code","source":["print (powerPlantDF.dtypes)"],"metadata":{},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":["We can examine the data using the display() method."],"metadata":{}},{"cell_type":"code","source":["display(powerPlantDF)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["### Exercise 2(c)\n\nInstead of having [spark-csv](https://spark-packages.org/package/databricks/spark-csv) infer the types of the columns, we can specify the schema as a [DataType](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.DataType), which is a list of [StructField](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.StructType).\nYou can find a list of types in the [pyspark.sql.types](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#module-pyspark.sql.types) module. For our data, we will use [DoubleType()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType).\nFor example, to specify that a column's name and type, we use: `StructField(`_name_`,` _type_`, True)`. (The third parameter, `True`, signifies that the column is nullable.)\n\nCreate a custom schema for the power plant data."],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import *\n\n# Custom Schema for Power Plant\ncustomSchema = StructType([ \\\n    StructField(\"AT\", DoubleType()), \\\n    StructField(\"V\", DoubleType()), \\\n    StructField(\"AP\", DoubleType()), \\\n    StructField(\"RH\", DoubleType()), \\\n    StructField(\"PE\", DoubleType()), \\\n                          ])\n"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"customSchema","locked":false,"solution":true,"checksum":"22d64b03f4ada1ed5431184541d8c7fe","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":["# TEST\nassert_equal(set([f.name for f in customSchema.fields]), set(['AT', 'V', 'AP', 'RH', 'PE']), 'Incorrect column names in schema.')\nassert_equal(set([f.dataType for f in customSchema.fields]), set([DoubleType(), DoubleType(), DoubleType(), DoubleType(), DoubleType()]), 'Incorrect column types in schema.')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_customSchema","locked":true,"solution":false,"points":1,"checksum":"f5b2f778f29464633d9ef75118c2b2e0","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["### Exercise 2(d)\n\nNow, let's use the schema to read the data. To do this, we will modify the earlier `sqlContext.read.format` step. We can specify the schema by:\n- Adding `schema = customSchema` to the load method (use a comma and add it after the file name)\n- Removing the `inferschema='true'`option because we are explicitly specifying the schema"],"metadata":{}},{"cell_type":"code","source":["# TODO: Uncomment the line and use the schema you created above to load the data again.\naltPowerPlantDF = sqlContext.read.format('com.databricks.spark.csv').options(delimiter='\\t', header='true').load(\"file://\" + SparkFiles.getRootDirectory(), schema=customSchema)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"readCustomSchema","locked":false,"solution":true,"checksum":"8541009e8be940ef2eafedddd719a3ff","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":["# TEST\nexpected = set([(s, 'double') for s in ('AP', 'AT', 'PE', 'RH', 'V')])\nassert_equal(expected, set(altPowerPlantDF.dtypes), \"Incorrect schema for powerPlantDF\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_readCustomSchema","locked":true,"solution":false,"points":1,"checksum":"c16aca4de5d46e846256577c8fdcdb2b","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["Note that no Spark jobs are launched this time. That is because we specified the schema, so the [spark-csv](https://spark-packages.org/package/databricks/spark-csv) package does not have to read the data to infer the schema. We can use the [dtypes](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.dtypes) method to examine the names and types of the columns. They should be identical to the names and types of the columns that were earlier inferred from the data.\n\nWhen you run the following cell, data would not be read."],"metadata":{}},{"cell_type":"code","source":["print (altPowerPlantDF.dtypes)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["Now we can examine the data using the display() method. *Note that this operation will cause the data to be read and the DataFrame will be created.*"],"metadata":{}},{"cell_type":"code","source":["display(altPowerPlantDF)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["## Part 3: Explore Your Data\nNow that your data is loaded, the next step is to explore it and perform some basic analysis and visualizations.\n\nThis is a step that you should always perform **before** trying to fit a model to the data, as this step will often lead to important insights about your data."],"metadata":{}},{"cell_type":"markdown","source":["First, let's register our DataFrame as an SQL table named `power_plant`.  Because you may run this lab multiple times, we'll take the precaution of removing any existing tables first.\n\nWe can delete any existing `power_plant` SQL table using the SQL command: `DROP TABLE IF EXISTS power_plant` (we also need to to delete any Hive data associated with the table, which we can do with a Databricks file system operation).\n\nOnce any prior table is removed, we can register our DataFrame as a SQL table using [sqlContext.registerDataFrameAsTable()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.SQLContext.registerDataFrameAsTable).\n\n### 3(a)\n\n**ToDo:** Execute the prepared code in the following cell."],"metadata":{}},{"cell_type":"code","source":["# uncomment to delete hive data in databricks\n\nsqlContext.sql(\"DROP TABLE IF EXISTS power_plant\")\nsqlContext.registerDataFrameAsTable(powerPlantDF, \"power_plant\")"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["# Now that our DataFrame exists as a SQL table, we can explore it using SQL commands.\n\nTo execute SQL in a cell, we use the `%sql` operator. The following cell is an example of using SQL to query the rows of the SQL table.\n\n**IMPORTANT**: uncomment the code in this part to run in databricks, COMMENT OUT ALL THE UNNECESSARY CODE IN THIS PART to submit to gradescope.\n\n**NOTE**: `%sql` is a Databricks-only command. It calls `sqlContext.sql()` and passes the results to the Databricks-only `display()` function. These two statements are equivalent:\n\n`%sql SELECT * FROM power_plant`\n\n`display(sqlContext.sql(\"SELECT * FROM power_plant\"))`\n\n### 3(b)\n\n**ToDo**: Execute the prepared code in the following cell."],"metadata":{}},{"cell_type":"code","source":["%sql\nSELECT * FROM power_plant"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["### 3(c)\n\nUse the SQL `desc` command to describe the schema, by executing the following cell."],"metadata":{}},{"cell_type":"code","source":["%sql desc power_plant"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["**Schema Definition**\n\nOnce again, here's our schema definition:\n\n- AT = Atmospheric Temperature in C\n- V = Exhaust Vacuum Speed\n- AP = Atmospheric Pressure\n- RH = Relative Humidity\n- PE = Power Output\n\nPE is our label or target. This is the value we are trying to predict given the measurements.\n\n*Reference [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"],"metadata":{}},{"cell_type":"markdown","source":["Let's perform some basic statistical analyses of all the columns.\n\nWe can get the DataFrame associated with a SQL table by using the [sqlContext.table()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.table) method and passing in the name of the SQL table. Then, we can use the DataFrame [describe()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.describe) method with no arguments to compute some basic statistics for each column like count, mean, max, min and standard deviation."],"metadata":{}},{"cell_type":"code","source":["# uncomment to run in databricks\n# comment to submit to gradescope\n\n# df = sqlContext.table(\"power_plant\")\n# display(df.describe())"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["##Part 4: Visualize Your Data\n\nTo understand our data, we will look for correlations between features and the label.  This can be important when choosing a model.  E.g., if features and a label are linearly correlated, a linear model like Linear Regression can do well; if the relationship is very non-linear, more complex models such as Decision Trees can be better. We can use Databrick's built in visualization to view each of our predictors in relation to the label column as a scatter plot to see the correlation between the predictors and the label.\n\n### Exercise 4(a)\n\n** Add figures to the following: **\nLet's see if there is a corellation between Temperature and Power Output. We can use a dataframe consisting of only the Temperature (AT) and Power (PE) columns, and then use a scatter plot with Temperature on the X axis and Power on the Y axis to visualize the relationship (if any) between Temperature and Power.\n\n\nPerform the following steps:\n\n- Run the following cell\n- Click on the drop down next to the \"Bar chart\" icon and select \"Scatter\" to turn the table into a Scatter plot\n\n<img src=\"http://spark-mooc.github.io/web-assets/images/cs110x/change-plot-type-scatter.png\" style=\"border: 1px solid #999999\"/>\n\n- Click on \"Plot Options...\"\n- In the Values box, click on \"Temperature\" and drag it before \"Power\", as shown below:\n\n<img src=\"http://spark-mooc.github.io/web-assets/images/cs110x/customize-plot-scatter.png\" style=\"border: 1px solid #999999\"/>\n\n- Apply your changes by clicking the Apply button\n- Increase the size of the graph by clicking and dragging the size control"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import col\ndisplay(powerPlantDF.select(powerPlantDF.AT.alias('Power'),powerPlantDF.PE.alias('Temperature')))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"scatterPlot_TemperaturePower","locked":false,"solution":true,"checksum":"474c2db6f9344419431cbd7bc85cf037","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["It looks like there is strong linear correlation between Temperature and Power Output.\n\n** ASIDE: A quick physics lesson**: This correlation is to be expected as the second law of thermodynamics puts a fundamental limit on the [thermal efficiency](https://en.wikipedia.org/wiki/Thermal_efficiency) of all heat-based engines. The limiting factors are:\n - The temperature at which the heat enters the engine \\\\( T_{H} \\\\)\n - The temperature of the environment into which the engine exhausts its waste heat \\\\( T_C \\\\)\n\nOur temperature measurements are the temperature of the environment. From [Carnot's theorem](https://en.wikipedia.org/wiki/Carnot%27s_theorem_%28thermodynamics%29), no heat engine working between these two temperatures can exceed the Carnot Cycle efficiency:\n\\\\[ n_{th} \\le 1 - \\frac{T_C}{T_H}  \\\\]\n\nNote that as the environmental temperature increases, the efficiency decreases -- _this is the effect that we see in the above graph._"],"metadata":{}},{"cell_type":"markdown","source":["### Exercise 4(b)\n\nUse DataFrame to create a scatter plot of Power(PE) as a function of ExhaustVacuum (V).\nName the y-axis \"Power\" and the x-axis \"ExhaustVacuum\""],"metadata":{}},{"cell_type":"code","source":["display(powerPlantDF.select(powerPlantDF.PE.alias('Power'),powerPlantDF.V.alias('ExhaustVacuum')))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"scatterPlot_VacuumPower","locked":false,"solution":true,"checksum":"1e1622f9c96094fab1be154b1d760b46","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":48},{"cell_type":"markdown","source":["Let's continue exploring the relationships (if any) between the variables and Power Output.\n\n### Exercise 4(c)\n\nUse DataFrame to create a scatter plot of Power(PE) as a function of Pressure (AP).\nName the y-axis \"Power\" and the x-axis \"Pressure\""],"metadata":{}},{"cell_type":"code","source":["display(powerPlantDF.select(powerPlantDF.PE.alias('Power'),powerPlantDF.AP.alias('Pressure')))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"scatterPlot_PressurePower","locked":false,"solution":true,"checksum":"e1b9d03f958242f11c54c74d3aa5e339","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":50},{"cell_type":"markdown","source":["### Exercise 4(d)\n\nUse DataFrame to create a scatter plot of Power(PE) as a function of Humidity (RH).\nName the y-axis \"Power\" and the x-axis \"Humidity\""],"metadata":{}},{"cell_type":"code","source":["display(powerPlantDF.select(powerPlantDF.PE.alias('Power'),powerPlantDF.RH.alias('Humidity')))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"scatterPlot_HumidityPower","locked":false,"solution":true,"checksum":"9a88c82a7a5556c6a1d7633b4a93c978","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":["##Part 5: Data Preparation\n\nThe next step is to prepare the data for machine learning. Since all of this data is numeric and consistent this is a simple and straightforward task.\n\nThe goal is to use machine learning to determine a function that yields the output power as a function of a set of predictor features. The first step in building our ML pipeline is to convert the predictor features from DataFrame columns to Feature Vectors using the [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler) method.\n\nThe VectorAssembler is a transformer that combines a given list of columns into a single vector column. It is useful for combining raw features and features generated by different feature transformers into a single feature vector, in order to train ML models like logistic regression and decision trees. VectorAssembler takes a list of input column names (each is a string) and the name of the output column (as a string).\n\n### Exercise 5(a)\n\n- Read the Spark documentation and useage examples for [VectorAssembler](https://spark.apache.org/docs/1.6.2/ml-features.html#vectorassembler)\n- Convert the `power_plant` SQL table into a DataFrame named `dataset`\n- Set the vectorizer's input columns to a list of the four columns of the input DataFrame: `[\"AT\", \"V\", \"AP\", \"RH\"]`\n- Set the vectorizer's output column name to `\"features\"`"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorAssembler\n\ndatasetDF = sqlContext.sql(\"SELECT * FROM power_plant\")\n\nvectorizer = VectorAssembler()\nvectorizer.setInputCols([\"AT\",\"V\",\"AP\",\"RH\"])\nvectorizer.setOutputCol(\"features\")"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"dataPreparation","locked":false,"solution":true,"checksum":"812b384544e3c5923bee208004f10c75","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":54},{"cell_type":"code","source":["# TEST\nassert_equal(set(vectorizer.getInputCols()), {\"AT\", \"V\", \"AP\", \"RH\"}, \"Incorrect vectorizer input columns\")\nassert_equal(vectorizer.getOutputCol(), \"features\", \"Incorrect vectorizer output column\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_dataPreparation","locked":true,"solution":false,"points":1,"checksum":"00dd17b9216483e4f10138f2d6868806","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":55},{"cell_type":"markdown","source":["##Part 6: Data Modeling\nNow let's model our data to predict what the power output will be given a set of sensor readings\n\nOur first model will be based on simple linear regression since we saw some linear patterns in our data based on the scatter plots during the exploration stage.\n\nWe need a way of evaluating how well our linear regression model predicts power output as a function of input parameters. We can do this by splitting up our initial data set into a _Training Set_ used to train our model and a _Test Set_ used to evaluate the model's performance in giving predictions. We can use a DataFrame's [randomSplit()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) method to split our dataset. The method takes a list of weights and an optional random seed. The seed is used to initialize the random number generator used by the splitting function. \nHowever, in this part we just slice the dataframe by index and we will use randomSplit() in later parts.\n\n### Exercise 6(a)\n\nUse [monotonically_increasing_id()](https://spark.apache.org/docs/2.1.0/api/python/pyspark.sql.html#pyspark.sql.functions.monotonically_increasing_id) method to create index for `datasetDF` and divide up it into a trainingSetDF (80% of the input DataFrame) and a testSetDF (20% of the input DataFrame). Then cache each DataFrame in memory to maximize performance."],"metadata":{}},{"cell_type":"code","source":["# We'll hold out 20% of our data for testing and leave 80% for training\nimport pyspark.sql.functions as f\nindexDF = datasetDF.withColumn('index', f.monotonically_increasing_id())\nsplit100 = indexDF.count() #indexDF.randomSplit([1.0], 123456)\nsplit20 = int(split100*0.2) #indexDF.randomSplit([0.2], 123456)\nsplit80 = int(split100*0.8)\n\n\nsplit20DF = indexDF.sort(['index']).limit(split20).drop('index')\nsplit80DF = indexDF.sort(['index'], ascending=False).limit(split80).drop('index')\n\n# Let's cache these datasets for performance\ntestSetDF = split20DF.cache()\ntrainingSetDF = split80DF.cache()"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"randomSplit","locked":false,"solution":true,"checksum":"ea142cabad8c11024589310d4094f63e","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":57},{"cell_type":"code","source":["# TEST\nassert_equal(trainingSetDF.count(), 38272, \"Incorrect size for training data set\")\nassert_equal(testSetDF.count(), 9568, \"Incorrect size for test data set\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_randomSplit","locked":true,"solution":false,"points":1,"checksum":"061df16dbd36890148e2b416e8464a7f","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":["Next we'll create a Linear Regression Model and use the built in help to identify how to train it. See API details for [Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) in the ML guide.\n\n### Exercise 6(b)\n\n- Read the documentation and examples for [Linear Regression](https://spark.apache.org/docs/1.6.2/ml-classification-regression.html#linear-regression)\n- Run the next cell"],"metadata":{}},{"cell_type":"code","source":["# ***** LINEAR REGRESSION MODEL ****\n\nfrom pyspark.ml.regression import LinearRegression\nfrom pyspark.ml.regression import LinearRegressionModel\nfrom pyspark.ml import Pipeline\n\n# Let's initialize our linear regression learner\nlr = LinearRegression()\n\n# We use explain params to dump the parameters we can use\nprint(lr.explainParams())"],"metadata":{},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":["The cell below is based on the [Spark ML Pipeline API for Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression).\n\nThe first step is to set the parameters for the method:\n- Set the name of the prediction column to \"Predicted_PE\"\n- Set the name of the label column to \"PE\"\n- Set the maximum number of iterations to 100\n- Set the regularization parameter to 0.1\n\nNext, we create the [ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Pipeline) and set the stages to the Vectorizer and Linear Regression learner we created earlier.\n\nFinally, we create a model by training on `trainingSetDF`.\n\n### Exercise 6(c)\n\n- Read the [Linear Regression](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression) documentation\n- Run the next cell, and be sure you understand what's going on."],"metadata":{}},{"cell_type":"code","source":["# Now we set the parameters for the method\nlr.setPredictionCol(\"Predicted_PE\")\\\n  .setLabelCol(\"PE\")\\\n  .setMaxIter(100)\\\n  .setRegParam(0.1)\n\n\n# We will use the new spark.ml pipeline API. If you have worked with scikit-learn this will be very familiar.\nlrPipeline = Pipeline()\n\nlrPipeline.setStages([vectorizer, lr])\n\n# Let's first train on the entire dataset to see what we get\nlrModel = lrPipeline.fit(trainingSetDF)"],"metadata":{},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":["From the Wikipedia article on [Linear Regression](https://en.wikipedia.org/wiki/Linear_regression):\n> In statistics, linear regression is an approach for modeling the relationship between a scalar dependent variable \\\\( y \\\\) and one or more explanatory variables (or independent variables) denoted \\\\(X\\\\). In linear regression, the relationships are modeled using linear predictor functions whose unknown model parameters are estimated from the data. Such models are called linear models.\n\nLinear regression has many practical uses. Most applications fall into one of the following two broad categories:\n  - If the goal is prediction, or forecasting, or error reduction, linear regression can be used to fit a predictive model to an observed data set of \\\\(y\\\\) and \\\\(X\\\\) values. After developing such a model, if an additional value of \\\\(X\\\\) is then given without its accompanying value of \\\\(y\\\\), the fitted model can be used to make a prediction of the value of \\\\(y\\\\).\n  - Given a variable \\\\(y\\\\) and a number of variables \\\\( X_1 \\\\), ..., \\\\( X_p \\\\) that may be related to \\\\(y\\\\), linear regression analysis can be applied to quantify the strength of the relationship between \\\\(y\\\\) and the \\\\( X_j\\\\), to assess which \\\\( X_j \\\\) may have no relationship with \\\\(y\\\\) at all, and to identify which subsets of the \\\\( X_j \\\\) contain redundant information about \\\\(y\\\\).\n\nWe are interested in both uses, as we would like to predict power output as a function of the input variables, and we would like to know which input variables are weakly or strongly correlated with power output.\n\nSince Linear Regression is simply a Line of best fit over the data that minimizes the square of the error, given multiple input dimensions we can express each predictor as a line function of the form:\n\n\\\\[ y = a + b x_1 + b x_2 + b x_i ... \\\\]\n\nwhere \\\\(a\\\\) is the intercept and the \\\\(b\\\\) are the coefficients.\n\nTo express the coefficients of that line we can retrieve the Estimator stage from the PipelineModel and express the weights and the intercept for the function.\n\n### Exercise 6(d)\n\nRun the next cell. Ensure that you understand what's going on."],"metadata":{}},{"cell_type":"code","source":["# The intercept is as follows:\nintercept = lrModel.stages[1].intercept\n\n# The coefficents (i.e., weights) are as follows:\nweights = lrModel.stages[1].coefficients\n\n# Create a list of the column names (without PE)\nfeaturesNoLabel = [col for col in datasetDF.columns if col != \"PE\"]\n\n# Merge the weights and labels\n# Sort the coefficients from greatest absolute weight most to the least absolute weight\ncoefficents = sorted(zip(weights, featuresNoLabel), key=lambda tup: abs(tup[0]), reverse=True)\n\nequation = \"y = {intercept}\".format(intercept=intercept)\nvariables = []\nfor x in coefficents:\n    weight = abs(x[0])\n    name = x[1]\n    symbol = \"+\" if (x[0] > 0) else \"-\"\n    equation += (\" {} ({} * {})\".format(symbol, weight, name))\n\n# Finally here is our equation\nprint(\"Linear Regression Equation: \" + equation)"],"metadata":{},"outputs":[],"execution_count":64},{"cell_type":"markdown","source":["Recall **Part 4: Visualize Your Data** when we visualized each predictor against Power Output using a Scatter Plot, does the final equation seems logical given those visualizations?\n\n### Exercise 6(e)\n\nNow let's see what our predictions look like given this model. We apply our Linear Regression model to the 20% of the data that we split from the input dataset. The output of the model will be a predicted Power Output column named \"Predicted_PE\".\n\n- Run the next cell\n- Scroll through the resulting table and notice how the values in the Power Output (PE) column compare to the corresponding values in the predicted Power Output (Predicted_PE) column"],"metadata":{}},{"cell_type":"code","source":["# Apply our LR model to the test data and predict power output\npredictionsAndLabelsDF = lrModel.transform(testSetDF).select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\")\n\ndisplay(predictionsAndLabelsDF)"],"metadata":{},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":["From a visual inspection of the predictions, we can see that they are close to the actual values.\n\nHowever, we would like a scientific measure of how well the Linear Regression model is performing in accurately predicting values. To perform this measurement, we can use an evaluation metric such as [Root Mean Squared Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation) (RMSE) to validate our Linear Regression model.\n\nRSME is defined as follows: \\\\( RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n} (x_i - y_i)^2}{n}}\\\\) where \\\\(y_i\\\\) is the observed value and \\\\(x_i\\\\) is the predicted value\n\nRMSE is a frequently used measure of the differences between values predicted by a model or an estimator and the values actually observed. The lower the RMSE, the better our model.\n\nSpark ML Pipeline provides several regression analysis metrics, including [RegressionEvaluator()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator).\n\nAfter we create an instance of [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator), we set the label column name to \"PE\" and set the prediction column name to \"Predicted_PE\". We then invoke the evaluator on the predictions.\n\n### Exercise 6(f)\n\nRun the next cell and ensure that you understand what's going on."],"metadata":{}},{"cell_type":"code","source":["# Now let's compute an evaluation metric for our test dataset\nfrom pyspark.ml.evaluation import RegressionEvaluator\n\n# Create an RMSE evaluator using the label and predicted columns\nregEval = RegressionEvaluator(predictionCol=\"Predicted_PE\", labelCol=\"PE\", metricName=\"rmse\")\n\n# Run the evaluator on the DataFrame\nrmse = regEval.evaluate(predictionsAndLabelsDF)\n\nprint(\"Root Mean Squared Error: %.2f\" % rmse)"],"metadata":{},"outputs":[],"execution_count":68},{"cell_type":"markdown","source":["Another useful statistical evaluation metric is the coefficient of determination, denoted \\\\(R^2\\\\) or \\\\(r^2\\\\) and pronounced \"R squared\". It is a number that indicates the proportion of the variance in the dependent variable that is predictable from the independent variable and it provides a measure of how well observed outcomes are replicated by the model, based on the proportion of total variation of outcomes explained by the model. The coefficient of determination ranges from 0 to 1 (closer to 1), and the higher the value, the better our model.\n\nTo compute \\\\(r^2\\\\), we invoke the evaluator with  `regEval.metricName: \"r2\"`\n\n### Exercise 6(g)\n\nRun the next cell and ensure that you understand what's going on."],"metadata":{}},{"cell_type":"code","source":["# Now let's compute another evaluation metric for our test dataset\nr2 = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"r2: {0:.2f}\".format(r2))"],"metadata":{},"outputs":[],"execution_count":70},{"cell_type":"markdown","source":["Generally, assuming a Gaussian distribution of errors, a good model will have 68% of predictions within 1 RMSE and 95% within 2 RMSE of the actual value (see http://statweb.stanford.edu/~susan/courses/s60/split/node60.html).\n\nLet's examine the predictions and see if the RMSE meets this criteria.\n\nWe create a new DataFrame using [selectExpr()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr) to project a set of SQL expressions, and register the DataFrame as a SQL table using [registerTempTable()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.sql.html#pyspark.sql.DataFrame.registerTempTable).\n\n### Exercise 6(h)\n\nRun the next cell and ensure that you understand what's going on."],"metadata":{}},{"cell_type":"code","source":["# First we remove the table if it already exists\nsqlContext.sql(\"DROP TABLE IF EXISTS Power_Plant_RMSE_Evaluation\")\n\n# Next we calculate the residual error and divide it by the RMSE\npredictionsAndLabelsDF.selectExpr(\"PE\", \"Predicted_PE\", \"PE - Predicted_PE Residual_Error\", \"(PE - Predicted_PE) / {} Within_RSME\".format(rmse)).registerTempTable(\"Power_Plant_RMSE_Evaluation\")"],"metadata":{},"outputs":[],"execution_count":72},{"cell_type":"markdown","source":["We can use SQL to explore the `Power_Plant_RMSE_Evaluation` table. First let's look at at the table using a SQL SELECT statement.\n\n### Exercise 6(i)\n\nRun the next cell and ensure that you understand what's going on."],"metadata":{}},{"cell_type":"code","source":["# %sql SELECT * from Power_Plant_RMSE_Evaluation"],"metadata":{},"outputs":[],"execution_count":74},{"cell_type":"markdown","source":["Now we can display the RMSE as a Histogram.\n\n### Exercise 6(j)\n\nPerform the following steps:\n\n- Run the following cell\n- Click on the drop down next to the \"Bar chart\" icon a select \"Histogram\" to turn the table into a Histogram plot\n\n<img src=\"http://spark-mooc.github.io/web-assets/images/cs110x/change-plot-type-histogram.png\" style=\"border: 1px solid #999999\"/>\n\n\n- Click on \"Plot Options...\"\n- In the \"All Fields:\" box, click on \"&lt;id&gt;\" and drag it into the \"Keys:\" box\n- Change the \"Aggregation\" to \"COUNT\"\n\n<img src=\"http://spark-mooc.github.io/web-assets/images/cs110x/customize-plot-histogram.png\" style=\"border: 1px solid #999999\"/>\n\n- Apply your changes by clicking the Apply button\n- Increase the size of the graph by clicking and dragging the size control\n\nNotice that the histogram clearly shows that the RMSE is centered around 0 with the vast majority of the error within 2 RMSEs.\n\n**IMPORTANT**: UNCOMMENT TO RUN IN DATABRICKS, COMMENT OUT SQL TO SUBMIT TO GRADESCOPE"],"metadata":{}},{"cell_type":"code","source":["# %sql -- Now we can display the RMSE as a Histogram\n# SELECT Within_RSME  from Power_Plant_RMSE_Evaluation"],"metadata":{},"outputs":[],"execution_count":76},{"cell_type":"markdown","source":["Using a complex SQL SELECT statement, we can count the number of predictions within + or - 1.0 and + or - 2.0 and then display the results as a pie chart.\n\n### Exercise 6(k)\n\nPerform the following steps:\n\n  - Run the following cell\n  - Click on the drop down next to the \"Bar chart\" icon a select \"Pie\" to turn the table into a Pie Chart plot\n  - Increase the size of the graph by clicking and dragging the size control"],"metadata":{}},{"cell_type":"code","source":["# %sql SELECT case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1\n#             when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3\n#        end RSME_Multiple, COUNT(*) AS count\n# FROM Power_Plant_RMSE_Evaluation\n# GROUP BY case when Within_RSME <= 1.0 AND Within_RSME >= -1.0 then 1  when  Within_RSME <= 2.0 AND Within_RSME >= -2.0 then 2 else 3 end"],"metadata":{},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":["From the pie chart, we can see that 68% of our test data predictions are within 1 RMSE of the actual values, and 97% (68% + 29%) of our test data predictions are within 2 RMSE. So the model is pretty decent. Let's see if we can tune the model to improve it further."],"metadata":{}},{"cell_type":"markdown","source":["##Part 7: Tuning and Evaluation\n\nNow that we have a model with all of the data let's try to make a better model by tuning over several parameters. The process of tuning a model is known as [Model Selection](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#module-pyspark.ml.tuning) or [Hyperparameter Tuning](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#module-pyspark.ml.tuning), and Spark ML Pipeline makes the tuning process very simple and easy.\n\nAn important task in ML is model selection, or using data to find the best model or parameters for a given task. This is also called tuning. Tuning may be done for individual Estimators such as [LinearRegression](https://spark.apache.org/docs/1.6.2/ml-classification-regression.html#linear-regression), or for entire Pipelines which include multiple algorithms, featurization, and other steps. Users can tune an entire Pipeline at once, rather than tuning each element in the Pipeline separately.\n\nSpark ML Pipeline supports model selection using tools such as [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator), which requires the following items:\n  - [Estimator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Estimator): algorithm or Pipeline to tune\n  - [Set of ParamMaps](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder): parameters to choose from, sometimes called a _parameter grid_ to search over\n  - [Evaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator): metric to measure how well a fitted Model does on held-out test data\n\nAt a high level, model selection tools such as [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) work as follows:\n  - They split the input data into separate training and test datasets.\n  - For each (training, test) pair, they iterate through the set of ParamMaps:\n    - For each [ParamMap](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder), they fit the [Estimator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Estimator) using those parameters, get the fitted Model, and evaluate the Model's performance using the [Evaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator).\n  - They select the Model produced by the best-performing set of parameters.\n\nThe [Evaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.Evaluator) can be a [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) for regression problems. To help construct the parameter grid, users can use the [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) utility.\n\nNote that cross-validation over a grid of parameters is expensive. For example, in the next cell, the parameter grid has 10 values for [lr.regParam](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.LinearRegression.regParam), and [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) uses 3 folds. This multiplies out to (10 x 3) = 30 different models being trained. In realistic settings, it can be common to try many more parameters (e.g., multiple values for multiple parameters) and use more folds (_k_ = 3 and _k_ = 10 are common). In other words, using [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) can be very expensive. However, it is also a well-established method for choosing parameters which is more statistically sound than heuristic hand-tuning.\n\nWe perform the following steps:\n  - Create a [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) using the Pipeline and [RegressionEvaluator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator) that we created earlier, and set the number of folds to 3\n  - Create a list of 10 regularization parameters\n  - Use [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to build a parameter grid with the regularization parameters and add the grid to the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)\n  - Run the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) to find the parameters that yield the best model (i.e., lowest RMSE) and return the best model.\n\n### Exercise 7(a)\n\nRun the next cell. _Note that it will take some time to run the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) as it will run almost 200 Spark jobs_"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\n# We can reuse the RegressionEvaluator, regEval, to judge the model based on the best Root Mean Squared Error\n# Let's create our CrossValidator with 3 fold cross validation\ncrossval = CrossValidator(estimator=lrPipeline, evaluator=regEval, numFolds=3)\n\n# Let's tune over our regularization parameter from 0.01 to 0.10\nregParam = [x / 100.0 for x in range(1, 11)]\n\n# We'll create a paramter grid using the ParamGridBuilder, and add the grid to the CrossValidator\nparamGrid = (ParamGridBuilder()\n             .addGrid(lr.regParam, regParam)\n             .build())\ncrossval.setEstimatorParamMaps(paramGrid)\n\n# Now let's find and return the best model\ncvModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{},"outputs":[],"execution_count":81},{"cell_type":"markdown","source":["Now that we have tuned our Linear Regression model, let's see what the new RMSE and \\\\(r^2\\\\) values are versus our intial model.\n\n### Exercise 7(b)\n\nComplete and run the next cell."],"metadata":{}},{"cell_type":"code","source":["# Now let's use cvModel to compute an evaluation metric for our test dataset: testSetDF\n# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\n# Now let's compute the r2 evaluation metric for our test dataset\n\npredictionsAndLabelsDF = cvModel.transform(testSetDF).select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\")\nrmseNew = regEval.evaluate(predictionsAndLabelsDF)\nr2New = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"Original Root Mean Squared Error: {0:2.2f}\".format(rmse))\nprint(\"New Root Mean Squared Error: {0:2.2f}\".format(rmseNew))\nprint(\"Old r2: {0:2.2f}\".format(r2))\nprint(\"New r2: {0:2.2f}\".format(r2New))\n"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"linearRegression","locked":false,"solution":true,"checksum":"c71980e9838cd90d9ddfcd134140395e","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":83},{"cell_type":"code","source":["# TEST\nassert_equal(round(rmse, 2), 4.56, \"Incorrect value for rmse\")\nassert_equal(round(rmseNew, 2), 4.56, \"Incorrect value for rmseNew\")\nassert_equal(round(r2, 2), 0.93, \"Incorrect value for r2\")\nassert_equal(round(r2New, 2), 0.93, \"Incorrect value for r2New\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_linearRegression","locked":true,"solution":false,"points":2,"checksum":"66a2a434ec062589b857d1bfdb110e0a","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":["So our initial untuned and tuned linear regression models are identical. Let's look at the regularization parameter that the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) has selected.\n\nRecall that the orginal regularization parameter we used was 0.01.\n\n**NOTE**: The ML Python API currently doesn't provide a way to query the regularization parameter, so we cheat, by \"reaching through\" to the JVM version of the API."],"metadata":{}},{"cell_type":"code","source":["print(\"Regularization parameter of the best model: {0:.2f}\".format(cvModel.stages[-1]._java_obj.parent().getRegParam()))"],"metadata":{},"outputs":[],"execution_count":86},{"cell_type":"markdown","source":["Given that the only linearly correlated variable is Temperature, it makes sense try another Machine Learning method such as [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree_learning) to handle non-linear data and see if we can improve our model.\n\n[Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning) uses a [Decision Tree](https://en.wikipedia.org/wiki/Decision_tree) as a predictive model which maps observations about an item to conclusions about the item's target value. It is one of the predictive modelling approaches used in statistics, data mining and machine learning. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.\n\nSpark ML Pipeline provides [DecisionTreeRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) as an implementation of [Decision Tree Learning](https://en.wikipedia.org/wiki/Decision_tree_learning).\n\nThe cell below is based on the [Spark ML Pipeline API for Decision Tree Regressor](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor).\n\n### Exercise 7(c)\n\n- Read the [Decision Tree Regressor](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) documentation\n- In the next cell, create a [DecisionTreeRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor)\n\n- The next step is to set the parameters for the method (we do this for you):\n  - Set the name of the prediction column to \"Predicted_PE\"\n  - Set the name of the features column to \"features\"\n  - Set the maximum number of bins to 100\n\n- Create the [ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Pipeline) and set the stages to the Vectorizer we created earlier and [DecisionTreeRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.DecisionTreeRegressor) learner we just created."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import DecisionTreeRegressor\n\ndt = DecisionTreeRegressor()\ndt.setLabelCol(\"PE\")\\\n  .setPredictionCol(\"Predicted_PE\")\\\n  .setFeaturesCol(\"features\")\\\n  .setMaxBins(100)\ndtPipeline = Pipeline()\ndtPipeline.setStages([vectorizer, dt])"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"buildDecisionTree","locked":false,"solution":true,"checksum":"772a109d48d9609b4507fc90869df606","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":["# TEST\nassert_equal(dtPipeline.getStages()[0].__class__.__name__, 'VectorAssembler', \"Incorrect pipeline stage 0\")\nassert_equal(dtPipeline.getStages()[1].__class__.__name__, 'DecisionTreeRegressor', \"Incorrect pipeline stage 0\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_buildDecisionTree","locked":true,"solution":false,"points":2,"checksum":"f3c7edd648e6a4f4becaaed7458120b9","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":89},{"cell_type":"markdown","source":["Instead of guessing what parameters to use, we will use [Model Selection](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#module-pyspark.ml.tuning) or [Hyperparameter Tuning](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#module-pyspark.ml.tuning) to create the best model.\n\nWe can reuse the exiting [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) by replacing the Estimator with our new `dtPipeline` (the number of folds remains 3).\n\n### Exercise 7(d)\n\n- Use [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to build a parameter grid with the parameter `dt.maxDepth` and a list of the values 2 and 3, and add the grid to the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)\n- Run the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) to find the parameters that yield the best model (i.e. lowest RMSE) and return the best model.\n\n_Note that it will take some time to run the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) as it will run almost 50 Spark jobs_"],"metadata":{}},{"cell_type":"code","source":["# Let's just reuse our CrossValidator with the new dtPipeline,  RegressionEvaluator regEval, and 3 fold cross validation\ncrossval.setEstimator(dtPipeline)\n\n# Let's tune over our dt.maxDepth parameter on the values 2 and 3, create a paramter grid using the ParamGridBuilder\n# Add the grid to the CrossValidator\n# Now let's find and return the best model\n\nparamGrid = ParamGridBuilder().addGrid(dt.maxDepth, [2,3]).build()\ncrossval.setEstimatorParamMaps(paramGrid)\ndtModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"bestModel_decisionTree","locked":false,"solution":true,"checksum":"ba5108fcc7b74526a161efc85067d721","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":["# TEST\nassert_equal(dtModel.stages[0].__class__.__name__, 'VectorAssembler', \"Incorrect pipeline stage 0\")\nassert_equal(dtModel.stages[1].__class__.__name__, 'DecisionTreeRegressionModel', \"Incorrect pipeline stage 0\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_bestModel_decisionTree","locked":true,"solution":false,"points":2,"checksum":"df57d79216099962546e2203ff5ffc9e","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":92},{"cell_type":"markdown","source":["### Exercise 7(e)\n\nNow let's see how our tuned DecisionTreeRegressor model's RMSE and \\\\(r^2\\\\) values compare to our tuned LinearRegression model.\n\nComplete and run the next cell."],"metadata":{}},{"cell_type":"code","source":["# Now let's use dtModel to compute an evaluation metric for our test dataset: testSetDF\n# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\n# Now let's compute the r2 evaluation metric for our test dataset\n\npredictionsAndLabelsDF = dtModel.transform(testSetDF).select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\")\nrmseDT = regEval.evaluate(predictionsAndLabelsDF)\nr2DT = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"LR Root Mean Squared Error: {0:.2f}\".format(rmseNew))\nprint(\"DT Root Mean Squared Error: {0:.2f}\".format(rmseDT))\nprint(\"LR r2: {0:.2f}\".format(r2New))\nprint(\"DT r2: {0:.2f}\".format(r2DT))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"evaluateDecisionTree","locked":false,"solution":true,"checksum":"342e715dd75adb150b1ff0bd0eebc57e","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":["# TEST\nassert_true(abs(round(rmseDT, 2) - 5.13) < 0.1, \"Incorrect value for rmseDT\")\nassert_equal(round(r2DT, 2), 0.91, \"Incorrect value for r2DT\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_evaluateDecisionTree","locked":true,"solution":false,"points":2,"checksum":"692431010dfe05957be4cebaad34ce6a","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":95},{"cell_type":"markdown","source":["The line below will pull the Decision Tree model from the Pipeline and display it as an if-then-else string. Again, we have to \"reach through\" to the JVM API to make this one work.\n\n**ToDo**: Run the next cell"],"metadata":{}},{"cell_type":"code","source":["# print (dtModel.stages[-1]._java_obj.toDebugString())"],"metadata":{},"outputs":[],"execution_count":97},{"cell_type":"markdown","source":["So our DecisionTree has slightly worse RMSE than our LinearRegression model (LR: 4.56 vs DT: 5.13). Maybe we can try an [Ensemble Learning](https://en.wikipedia.org/wiki/Ensemble_learning) method such as [Gradient-Boosted Decision Trees](https://en.wikipedia.org/wiki/Gradient_boosting) to see if we can strengthen our model by using an ensemble of weaker trees with weighting to reduce the error in our model.\n\n[Random forests](https://en.wikipedia.org/wiki/Random_forest) or random decision tree forests are an ensemble learning method for regression that operate by constructing a multitude of decision trees at training time and outputting the class that is the mean prediction (regression) of the individual trees. Random decision forests correct for decision trees' habit of overfitting to their training set.\n\nSpark ML Pipeline provides [RandomForestRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor) as an implementation of [Random forests](https://en.wikipedia.org/wiki/Random_forest).\n\nThe cell below is based on the [Spark ML Pipeline API for Random Forest Regressor](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor).\n\n### Exercise 7(f)\n\n- Read the [Random Forest Regressor](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor) documentation\n- In the next cell, create a [RandomForestRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor)\n- The next step is to set the parameters for the method (we do this for you):\n  - Set the name of the prediction column to \"Predicted_PE\"\n  - Set the name of the features column to \"features\"\n  - Set the random number generator seed to 100088121L\n  - Set the maximum depth to 8\n  - Set the number of trees to 30\n- Create the [ML Pipeline](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.Pipeline) and set the stages to the Vectorizer we created earlier and [RandomForestRegressor()](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.regression.RandomForestRegressor) learner we just created."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.regression import RandomForestRegressor\n\nrf = RandomForestRegressor()\nrf.setLabelCol(\"PE\")\\\n  .setPredictionCol(\"Predicted_PE\")\\\n  .setFeaturesCol(\"features\")\\\n  .setSeed(100088121)\\\n  .setMaxDepth(8)\\\n  .setNumTrees(30)\nrfPipeline = Pipeline()\nrfPipeline.setStages([vectorizer, rf])"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"buildRandomForest","locked":false,"solution":true,"checksum":"636a7da704b5407747b9634e74f68e2c","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":99},{"cell_type":"code","source":["# TEST\nassert_equal(rfPipeline.getStages()[0].__class__.__name__, 'VectorAssembler', \"Stage 0 of pipeline is not correct\")\nassert_equal(rfPipeline.getStages()[1].__class__.__name__, 'RandomForestRegressor', \"Stage 1 of pipeline is not correct\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_buildRandomForest","locked":true,"solution":false,"points":2,"checksum":"3db04eb19d42d435cfaed6c64df05cbd","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":100},{"cell_type":"markdown","source":["As with Decision Trees, instead guessing what parameters to use, we will use [Model Selection](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#module-pyspark.ml.tuning) or [Hyperparameter Tuning](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#module-pyspark.ml.tuning) to create the best model.\n\nWe can reuse the existing [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) by replacing the Estimator with our new `rfPipeline` (the number of folds remains 3).\n\n### Exercise 7(g)\n\n- Use [ParamGridBuilder](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.ParamGridBuilder) to build a parameter grid with the parameter `rf.maxBins` and a list of the values 50 and 100, and add the grid to the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator)\n- Run the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) to find the parameters that yield the best model (i.e., lowest RMSE) and return the best model.\n\n_Note that it will take some time to run the [CrossValidator](https://spark.apache.org/docs/1.6.2/api/python/pyspark.ml.html#pyspark.ml.tuning.CrossValidator) as it will run almost 100 Spark jobs, and each job takes longer to run than the prior CrossValidator runs._"],"metadata":{}},{"cell_type":"code","source":["# Let's just reuse our CrossValidator with the new rfPipeline,  RegressionEvaluator regEval, and 3 fold cross validation\ncrossval.setEstimator(rfPipeline)\n\n# Let's tune over our rf.maxBins parameter on the values 50 and 100, create a parameter grid using the ParamGridBuilder\n# Add the grid to the CrossValidator\n# Now let's find and return the best model\n\nparamGrid = ParamGridBuilder().addGrid(rf.maxBins, [50, 100]).build()\ncrossval.setEstimatorParamMaps(paramGrid)\nrfModel = crossval.fit(trainingSetDF).bestModel"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"bestModel_randomForest","locked":false,"solution":true,"checksum":"80705b9b5748c690cd8cbe69e7b20a1e","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":102},{"cell_type":"code","source":["# TEST\nassert_equal(rfModel.stages[0].__class__.__name__, 'VectorAssembler', 'rfModel has incorrect stage 0')\nassert_equal(rfModel.stages[1].__class__.__name__, 'RandomForestRegressionModel', 'rfModel has incorrect stage 1')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_bestModel_randomForest","locked":true,"solution":false,"points":2,"checksum":"f3d5809cdb96e64ac111a2876ec7f338","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":103},{"cell_type":"markdown","source":["### Exercise 7(h)\n\nNow let's see how our tuned RandomForestRegressor model's RMSE and \\\\(r^2\\\\) values compare to our tuned LinearRegression and tuned DecisionTreeRegressor models.\n\nComplete and run the next cell."],"metadata":{}},{"cell_type":"code","source":["# Now let's use rfModel to compute an evaluation metric for our test dataset: testSetDF\n# Run the previously created RMSE evaluator, regEval, on the predictionsAndLabelsDF DataFrame\n# Now let's compute the r2 evaluation metric for our test dataset\n\npredictionsAndLabelsDF = rfModel.transform(testSetDF).select(\"AT\", \"V\", \"AP\", \"RH\", \"PE\", \"Predicted_PE\")\nrmseRF = regEval.evaluate(predictionsAndLabelsDF)\nr2RF = regEval.evaluate(predictionsAndLabelsDF, {regEval.metricName: \"r2\"})\n\nprint(\"LR Root Mean Squared Error: {0:.2f}\".format(rmseNew))\nprint(\"DT Root Mean Squared Error: {0:.2f}\".format(rmseDT))\nprint(\"RF Root Mean Squared Error: {0:.2f}\".format(rmseRF))\nprint(\"LR r2: {0:.2f}\".format(r2New))\nprint(\"DT r2: {0:.2f}\".format(r2DT))\nprint(\"RF r2: {0:.2f}\".format(r2RF))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"evaluateRandomForest","locked":false,"solution":true,"checksum":"cdbff3a29c7f7d21978955a545ee8904","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":["# TEST\nassert_true(abs(round(rmseRF, 2) - 3.35) < 0.05, \"Incorrect value for rmseDT\")\nassert_equal(round(r2RF, 2), 0.96, \"Incorrect value for r2RF\")"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_evaluateRandomForest","locked":true,"solution":false,"points":3,"checksum":"ab17b22afde405ca04ac5fa3ef7c5b17","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":106},{"cell_type":"markdown","source":["Note that the `r2` values are similar for all three. However, the RMSE for the Random Forest model is better."],"metadata":{}},{"cell_type":"markdown","source":["The line below will pull the Random Forest model from the Pipeline and display it as an if-then-else string.\n\n**ToDo**: Run the next cell"],"metadata":{}},{"cell_type":"code","source":["print (rfModel.stages[-1]._java_obj.toDebugString())"],"metadata":{},"outputs":[],"execution_count":109},{"cell_type":"markdown","source":["### Conclusion\n\nWow! So our best model is in fact our Random Forest tree model which uses an ensemble of 30 Trees with a depth of 8 to construct a better model than the single decision tree."],"metadata":{}},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":111},{"cell_type":"markdown","source":["# II. Linear Regression\n\nThis section covers a common supervised learning pipeline, using a subset of the [Million Song Dataset](http://labrosa.ee.columbia.edu/millionsong/) from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/YearPredictionMSD). Our goal is to train a linear regression model to predict the release year of a song given a set of audio features.\n\n## This section will cover:\n*  *Part 1:* Read and parse the initial dataset\n  * *Visualization 1:* Features\n  * *Visualization 2:* Shifting labels\n\n*  *Part 2:* Create and evaluate a baseline model\n  * *Visualization 3:* Predicted vs. actual\n\n*  *Part 3:* Train (via gradient descent) and evaluate a linear regression model\n  * *Visualization 4:* Training error\n\n*  *Part 4:* Train using SparkML and tune hyperparameters via grid search\n  * *Visualization 5:* Best model's predictions\n  * *Visualization 6:* Hyperparameter heat map\n\n*  *Part 5:* Add interactions between features\n\n> Note that, for reference, you can look up the details of:\n> * the relevant Spark methods in [Spark's RDD Python API](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD) and [Spark's DataFrame Python API](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)\n> * the relevant NumPy methods in the [NumPy Reference](http://docs.scipy.org/doc/numpy/reference/index.html)"],"metadata":{}},{"cell_type":"markdown","source":["## Part 1: Read and parse the initial dataset"],"metadata":{}},{"cell_type":"markdown","source":["### (1a) Load and check the data\n\nThe raw data is currently stored in text file.  We will start by storing this raw data in as a DataFrame, with each element of the DataFrame representing a data point as a comma-delimited string. Each string starts with the label (a year) followed by numerical audio features. Use the DataFrame [count method](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.count) to check how many data points we have.  Then use the [take method](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.take) to create and print out a list of the first 5 data points in their initial string format."],"metadata":{}},{"cell_type":"code","source":["from pyspark import SparkFiles\nurl = 'https://raw.githubusercontent.com/10605/data/master/hw2/millionsong.txt'\nsc.addFile(url)\nraw_data_df = sqlContext.read.load(\"file://\" + SparkFiles.get(\"millionsong.txt\"), 'text')"],"metadata":{},"outputs":[],"execution_count":115},{"cell_type":"code","source":["num_points = raw_data_df.count()\nprint (num_points)\nsample_points = raw_data_df.take(5)\nprint (sample_points)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"checkData","locked":false,"solution":true,"checksum":"ac944df2e96f0ba0b1e4b624a946e75e","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":116},{"cell_type":"code","source":["# TEST Load and check the data (1a)\n# from nose.tools import assert_equal, assert_true\nassert_equal(num_points, 6724, 'incorrect value for num_points')\nassert_equal(len(sample_points), 5, 'incorrect length for sample_points')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_checkData","locked":true,"solution":false,"points":1,"checksum":"02579224301a2a40c6fc4bd4ce51c99d","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":117},{"cell_type":"markdown","source":["### (1b) Using `LabeledPoint`\n\nIn MLlib, labeled training instances are stored using the [LabeledPoint](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint) object.  Write the `parse_points` function that takes, as input, a DataFrame of comma-separated strings. We'll pass it the `raw_data_df` DataFrame.\n\nIt should parse each row in the DataFrame into individual elements, using Spark's [select](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select) and [split](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.functions.split) methods.\n\nFor example, split `\"2001.0,0.884,0.610,0.600,0.474,0.247,0.357,0.344,0.33,0.600,0.425,0.60,0.419\"` into `['2001.0', '0.884', '0.610', '0.600', '0.474', '0.247', '0.357', '0.344', '0.33', '0.600', '0.425', '0.60', '0.419']`.\n\nThe first value in the resulting list (`2001.0` in the example, above) is the label. The remaining values (`0.884`, `0.610`, etc., in the example) are the features.\n\nAfter splitting each row, map it to a `LabeledPoint`. You'll have to step down to an RDD (using `.rdd`) or use a DataFrame user-defined function to convert to the `LabeledPoint` object. (See **Hint**, below.) If you step down to an RDD, you'll have to use [toDF()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.toDF) to convert back to a DataFrame.\n\nUse this new `parse_points` function to parse `raw_data_df`.  Then print out the features and label for the first training point, using the `features` and `label` attributes. Finally, calculate the number of features for this dataset.\n\n## Hint: Running Arbitrary Lambdas on a DataFrame\n\nTo solve this problem, you need a way to run your `parse_points` function on a DataFrame. There are two ways to do this, which we will illustrate with an extremely simple example.\n\nSuppose you have a DataFrame consisting of a first name and a last name, and you want to add a unique [SHA-256](https://en.wikipedia.org/wiki/Secure_Hash_Algorithm) hash to each row.\n\n```\ndf = sqlContext.createDataFrame([(\"John\", \"Smith\"), (\"Ravi\", \"Singh\"), (\"Julia\", \"Jones\")], (\"first_name\", \"last_name\"))\n```\n\nHere's a simple function to calculate such a hash, using Python's built-in `hashlib` library:\n\n```\ndef make_hash(first_name, last_name):\n    import hashlib\n    m = hashlib.sha256()\n    # Join the first name and last name by a blank and hash the resulting\n    # string.\n    full_name = ' '.join((first_name, last_name))\n    m.update(full_name)\n    return m.hexdigest()\n```\n\nOkay, that's great. But, how do we use it on our DataFrame? We can use a UDF:\n\n```\nfrom pyspark.sql.functions import udf\nu_make_hash = udf(make_hash)\ndf2 = df.select(df['*'], u_make_hash(df['first_name'], df['last_name']))\n# could run df2.show() here to prove it works\n```\n\nOr we can step down to an RDD, use a lambda to call `make_hash` and have the lambda return a `Row` object, which Spark can use to [\"infer\" a new DataFrame](http://spark.apache.org/docs/latest/sql-programming-guide.html#inferring-the-schema-using-reflection).\n\n```\nfrom pyspark.sql import Row\ndef make_hash_from_row(row):\n    hash = make_hash(row[0], row[1])\n    return Row(first_name=row[0], last_name=row[1], hash=hash)\n\ndf2 = (df.rdd\n         .map(lambda row: make_hash_from_row(row))\n         .toDF())\n```\n\nThese methods are roughly equivalent. You'll need to do something similar to convert _your_ `raw_data_df` DataFrame into a new DataFrame of `LabeledPoint` objects."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.regression import LabeledPoint\nimport numpy as np\n\n# Here is a sample raw data point:\n# '2001.0,0.884,0.610,0.600,0.474,0.247,0.357,0.344,0.33,0.600,0.425,0.60,0.419'\n# In this raw data point, 2001.0 is the label, and the remaining values are features"],"metadata":{},"outputs":[],"execution_count":119},{"cell_type":"code","source":["from pyspark.sql import functions as sql_functions\n\ndef parse_points(df):\n    \"\"\"Converts a DataFrame of comma separated unicode strings into a DataFrame of `LabeledPoints`.\n\n    Args:\n        df: DataFrame where each row is a comma separated unicode string. The first element in the string\n            is the label and the remaining elements are the features.\n\n    Returns:\n        DataFrame: Each row is converted into a `LabeledPoint`, which consists of a label and\n            features. To convert an RDD to a DataFrame, simply call toDF().\n    \"\"\"\n    def makeLabel(row):\n      comp = row['value'].split(',')\n      label = float(comp[0])\n      feat = [float(c) for c in comp[1:]]\n      return LabeledPoint(label, feat)\n    df2 = df.rdd.map(lambda row: makeLabel(row)).toDF()\n    return df2\n\nparsed_points_df = parse_points(raw_data_df)\nfirst_point_features = parsed_points_df.first()['features']\nfirst_point_label = parsed_points_df.first()['label']\nprint (first_point_features, first_point_label)\n\nd = len(first_point_features)\nprint (d)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"parsePoints","locked":false,"solution":true,"checksum":"18af903313b571c9d4647f239efc6148","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":120},{"cell_type":"code","source":["# TEST Using LabeledPoint (1b)\nassert_true(isinstance(first_point_label, float), 'label must be a float')\nexpectedX0 = [0.8841,0.6105,0.6005,0.4747,0.2472,0.3573,0.3441,0.3396,0.6009,0.4257,0.6049,0.4192]\nassert_true(np.allclose(expectedX0, first_point_features, 1e-4, 1e-4),\n                'incorrect features for firstPointFeatures')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_parsePoints","locked":true,"solution":false,"points":2,"checksum":"cb97514360c194564d0350d01ecc77d2","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":121},{"cell_type":"markdown","source":["### Visualization 1: Features\n\nFirst we will load and setup the visualization library. Then we will look at the raw features for 50 data points by generating a heatmap that visualizes each feature on a grey-scale and shows the variation of each feature across the 50 sample data points.  The features are all between 0 and 1, with values closer to 1 represented via darker shades of grey."],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\n# takeSample(withReplacement, num, [seed]) randomly selects num elements from the dataset with/without replacement, and has an\n# optional seed parameter that one can set for reproducible results\n\ndata_values = (parsed_points_df\n               .rdd\n               .map(lambda lp: lp.features.toArray())\n               .takeSample(False, 50, 47))\n\n# You can uncomment the line below to see randomly selected features.  These will be randomly\n# selected each time you run the cell because there is no set seed.  Note that you should run\n# this cell with the line commented out when answering the lab quiz questions.\n# data_values = (parsedPointsDF\n#                .rdd\n#                .map(lambda lp: lp.features.toArray())\n#                .takeSample(False, 50))\n\ndef prepare_plot(xticks, yticks, figsize=(10.5, 6), hideLabels=False, gridColor='#999999',\n                 gridWidth=1.0):\n    \"\"\"Template for generating the plot layout.\"\"\"\n    plt.close()\n    fig, ax = plt.subplots(figsize=figsize, facecolor='white', edgecolor='white')\n    ax.axes.tick_params(labelcolor='#999999', labelsize='10')\n    for axis, ticks in [(ax.get_xaxis(), xticks), (ax.get_yaxis(), yticks)]:\n        axis.set_ticks_position('none')\n        axis.set_ticks(ticks)\n        axis.label.set_color('#999999')\n        if hideLabels: axis.set_ticklabels([])\n    plt.grid(color=gridColor, linewidth=gridWidth, linestyle='-')\n    map(lambda position: ax.spines[position].set_visible(False), ['bottom', 'top', 'left', 'right'])\n    return fig, ax\n\n# generate layout and plot\nfig, ax = prepare_plot(np.arange(.5, 11, 1), np.arange(.5, 49, 1), figsize=(8,7), hideLabels=True,\n                       gridColor='#eeeeee', gridWidth=1.1)\nimage = plt.imshow(data_values,interpolation='nearest', aspect='auto', cmap=cm.Greys)\nfor x, y, s in zip(np.arange(-.125, 12, 1), np.repeat(-.75, 12), [str(x) for x in range(12)]):\n    plt.text(x, y, s, color='#999999', size='10')\nplt.text(4.7, -3, 'Feature', color='#999999', size='11'), ax.set_ylabel('Observation')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":123},{"cell_type":"markdown","source":["### (1c) Find the range\n\nNow let's examine the labels to find the range of song years.  To do this, find the smallest and largest labels in the `parsed_points_df`.\n\nWe will use the min and max functions that are native to the DataFrames, and thus can be optimized using Spark's Catalyst Optimizer and Project Tungsten (don't worry about the technical details). This code will run faster than simply using the native min and max functions in Python. Use [selectExpr](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.selectExpr) to retrieve the min and max label values."],"metadata":{}},{"cell_type":"code","source":["content_stats = (parsed_points_df\n                 .selectExpr('label'))\nmin_year = content_stats.selectExpr('min(label)').first()['min(label)']\nmax_year = content_stats.selectExpr('max(label)').first()['max(label)']\n\nprint (min_year, max_year)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"yearRange","locked":false,"solution":true,"checksum":"6584f0e1e8a22924f679329e3f492ebd","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":125},{"cell_type":"code","source":["# TEST Find the range (1c)\nassert_equal(len(parsed_points_df.first().features), 12,\n                  'unexpected number of features in sample point')\nsum_feat_two = parsed_points_df.rdd.map(lambda lp: lp.features[2]).sum()\nassert_true(np.allclose(sum_feat_two, 3158.96224351), 'parsedPointsDF has unexpected values')\nyear_range = max_year - min_year\nassert_true(year_range == 89, 'incorrect range for minYear to maxYear')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_yearRange","locked":true,"solution":false,"points":1,"checksum":"e250b74d7cc9ddf7f5ec7b335a560254","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":126},{"cell_type":"markdown","source":["### (1d) Shift labels\n\nAs we just saw, the labels are years in the 1900s and 2000s.  In learning problems, it is often natural to shift labels such that they start from zero.  Starting with `parsed_points_df`, create a new DataFrame in which the labels are shifted such that smallest label equals zero (hint: use `select`). After, use [withColumnRenamed](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.withColumnRenamed) to rename the appropriate columns to `features` and `label`."],"metadata":{}},{"cell_type":"code","source":["# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\n\nparsed_data_df = parsed_points_df.select(parsed_points_df.features, parsed_points_df.label-min_year)\nparsed_data_df = parsed_data_df.withColumnRenamed('(label - '+str(min_year)+')', 'label')\n# View the first point\n\nprint ('\\n{0}'.format(parsed_data_df.first()))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"shiftLabels","locked":false,"solution":true,"checksum":"51d009bdb215c1033ee38de1abb3dc1e","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":128},{"cell_type":"code","source":["# TEST Shift labels (1d)\nold_sample_features = parsed_points_df.first().features\nnew_sample_features = parsed_data_df.first().features\nassert_true(np.allclose(old_sample_features, new_sample_features),\n                'new features do not match old features')\nsum_feat_two = parsed_data_df.rdd.map(lambda lp: lp.features[2]).sum()\nassert_true(np.allclose(sum_feat_two, 3158.96224351), 'parsed_data_df has unexpected values')\nmin_year_new = parsed_data_df.groupBy().min('label').first()[0]\nmax_year_new = parsed_data_df.groupBy().max('label').first()[0]\nassert_true(min_year_new == 0, 'incorrect min year in shifted data')\nassert_true(max_year_new == 89, 'incorrect max year in shifted data')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_shiftLabels","locked":true,"solution":false,"points":2,"checksum":"c5d17a76219d65e81659b5bcb123cf5d","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":129},{"cell_type":"markdown","source":["### Visualization 2: Shifting labels\n\nWe will look at the labels before and after shifting them.  Both scatter plots below visualize tuples storing:\n\n* a label value and\n* the number of training points with this label.\n\nThe first scatter plot uses the initial labels, while the second one uses the shifted labels.  Note that the two plots look the same except for the labels on the x-axis."],"metadata":{}},{"cell_type":"code","source":["# get data for plot\nold_data = (parsed_points_df\n             .rdd\n             .map(lambda lp: (lp.label, 1))\n             .reduceByKey(lambda x, y: x + y)\n             .collect())\nx, y = zip(*old_data)\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(1920, 2050, 20), np.arange(0, 150, 20))\nplt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\nax.set_xlabel('Year'), ax.set_ylabel('Count')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":131},{"cell_type":"code","source":["# get data for plot\nnew_data = (parsed_data_df\n             .rdd\n             .map(lambda lp: (lp.label, 1))\n             .reduceByKey(lambda x, y: x + y)\n             .collect())\nx, y = zip(*new_data)\n\n# generate layout and plot data\nfig, ax = prepare_plot(np.arange(0, 120, 20), np.arange(0, 120, 20))\nplt.scatter(x, y, s=14**2, c='#d6ebf2', edgecolors='#8cbfd0', alpha=0.75)\nax.set_xlabel('Year (shifted)'), ax.set_ylabel('Count')\ndisplay(fig)\npass"],"metadata":{},"outputs":[],"execution_count":132},{"cell_type":"markdown","source":["### (1e) Training, validation, and test sets\n\nWe're almost done parsing our dataset, and our final task involves spliting the dataset into training, validation and test sets. Use the [randomSplit method](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit) with the specified weights and seed to create DataFrames storing each of these datasets. Next, cache each of these DataFrames, as we will be accessing them multiple times in the remainder of this lab. Finally, compute the size of each dataset and verify that the sum of their sizes equals the value computed in Part (1a)."],"metadata":{}},{"cell_type":"code","source":["# TODO: Uncomment the following lines and replace <FILL IN> with appropriate code\nweights = [.8, .1, .1]\nseed = 42\n\nparsed_train_data_df, parsed_val_data_df, parsed_test_data_df = parsed_data_df.randomSplit(weights, seed)\nparsed_train_data_df.cache()\nparsed_val_data_df.cache()\nparsed_test_data_df.cache()\nn_train = parsed_train_data_df.count()\nn_val = parsed_val_data_df.count()\nn_test = parsed_test_data_df.count()\n\nprint (n_train, n_val, n_test, n_train + n_val + n_test)\nprint (parsed_data_df.count())"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"splitData","locked":false,"solution":true,"checksum":"08696155af8ac49105f058f6bad84bee","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":134},{"cell_type":"code","source":["# TEST Training, validation, and test sets (1e)\nassert_equal(len(parsed_train_data_df.first().features), 12,\n                  'parsed_train_data_df has wrong number of features')\nsum_feat_two = (parsed_train_data_df\n                 .rdd\n                 .map(lambda lp: lp.features[2])\n                 .sum())\nsum_feat_three = (parsed_val_data_df\n                  .rdd\n                  .map(lambda lp: lp.features[3])\n                  .reduce(lambda x, y: x + y))\nsum_feat_four = (parsed_test_data_df\n                  .rdd\n                  .map(lambda lp: lp.features[4])\n                  .reduce(lambda x, y: x + y))\nassert_true(np.allclose([sum_feat_two, sum_feat_three, sum_feat_four],\n                            2526.87757656, 297.340394298, 184.235876654),\n                'parsed Train, Val, Test data has unexpected values')\nassert_true(n_train + n_val + n_test == 6724, 'unexpected Train, Val, Test data set size')\nassert_equal(n_train, 5405, 'unexpected value for nTrain')\nassert_equal(n_val, 644, 'unexpected value for nVal')\nassert_equal(n_test, 675, 'unexpected value for nTest')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_splitData","locked":true,"solution":false,"points":2,"checksum":"0c01352a17a30277f19d98c69a1fc259","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":135},{"cell_type":"markdown","source":["## Part 2: Create and evaluate a baseline model"],"metadata":{}},{"cell_type":"markdown","source":["### (2a) Average label\n\nA very simple yet natural baseline model is one where we always make the same prediction independent of the given data point, using the average label in the training set as the constant prediction value.  Compute this value, which is the average (shifted) song year for the training set.  Use `selectExpr` and `first()` from the [DataFrame API](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame)."],"metadata":{}},{"cell_type":"code","source":["average_train_year = (parsed_train_data_df\n                        .selectExpr('avg(label)').first()['avg(label)'])\n\n\nprint(sum(parsed_train_data_df.rdd.map(lambda x:x.label).collect()), parsed_train_data_df.count())\nprint (average_train_year)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"baseline","locked":false,"solution":true,"checksum":"b313dc323fdd2899ded2b8edeaa5c216","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":138},{"cell_type":"code","source":["# TEST Average label (2a)\nassert_true(np.allclose(average_train_year, 53.68140610545791),\n                'incorrect value for average_train_year')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_baseline","locked":true,"solution":false,"points":2,"checksum":"4b05b7381d651c0d44284f00e117936f","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":139},{"cell_type":"markdown","source":["### (2b) Root mean squared error\n\nWe naturally would like to see how well this naive baseline performs.  We will use root mean squared error ([RMSE](http://en.wikipedia.org/wiki/Root-mean-square_deviation)) for evaluation purposes.  Using [Regression Evaluator](http://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.RegressionEvaluator),  compute the RMSE given a dataset of _(prediction, label)_ tuples."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import RegressionEvaluator\n\npreds_and_labels = [(1., 3.), (2., 1.), (2., 2.)]\npreds_and_labels_df = sqlContext.createDataFrame(preds_and_labels, [\"prediction\", \"label\"])\n\n\nevaluator = RegressionEvaluator(predictionCol='prediction', labelCol='label')\n\ndef calc_RMSE(dataset):\n    \"\"\"Calculates the root mean squared error for an dataset of (prediction, label) tuples.\n\n    Args:\n        dataset (DataFrame of (float, float)): A `DataFrame` consisting of (prediction, label) tuples.\n\n    Returns:\n        float: The square root of the mean of the squared errors.\n    \"\"\"\n    \n    return evaluator.evaluate(dataset)\n\nexample_rmse = calc_RMSE(preds_and_labels_df)\nprint (example_rmse)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"RMSE_toy","locked":false,"solution":true,"checksum":"d7ae1ffc28c6f06fbc0dd84b6cd6aec6","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":141},{"cell_type":"code","source":["# TEST Root mean squared error (2b)\nassert_true(np.allclose(example_rmse, 1.29099444874), 'incorrect value for exampleRMSE')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_RMSE_toy","locked":true,"solution":false,"points":2,"checksum":"a4b39981db2a936db74517b3fe8fbb43","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":142},{"cell_type":"markdown","source":["### (2c) Training, validation and test RMSE\n\nNow let's calculate the training, validation and test RMSE of our baseline model. To do this, first create DataFrames of _(prediction, label)_ tuples for each dataset, and then call `calc_RMSE()`. Note that each RMSE can be interpreted as the average prediction error for the given dataset (in terms of number of years). You can use [createDataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.SQLContext.createDataFrame) to make a DataFrame with the column names of \"prediction\" and \"label\" from an RDD."],"metadata":{}},{"cell_type":"code","source":["preds_and_labels_train = parsed_train_data_df.rdd.map(lambda x:(average_train_year, x.label))\npreds_and_labels_train_df = sqlContext.createDataFrame(preds_and_labels_train, [\"prediction\", \"label\"])\nrmse_train_base = calc_RMSE(preds_and_labels_train_df)\n\npreds_and_labels_val = parsed_val_data_df.rdd.map(lambda x:(average_train_year, x.label))\npreds_and_labels_val_df = sqlContext.createDataFrame(preds_and_labels_val, [\"prediction\", \"label\"])\nrmse_val_base = calc_RMSE(preds_and_labels_val_df)\n\npreds_and_labels_test = parsed_test_data_df.rdd.map(lambda x:(average_train_year ,x.label))\npreds_and_labels_test_df = sqlContext.createDataFrame(preds_and_labels_test, [\"prediction\", \"label\"])\nrmse_test_base = calc_RMSE(preds_and_labels_test_df)\n\nprint ('Baseline Train RMSE = {0:.3f}'.format(rmse_train_base))\nprint ('Baseline Validation RMSE = {0:.3f}'.format(rmse_val_base))\nprint ('Baseline Test RMSE = {0:.3f}'.format(rmse_test_base))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"RMSE","locked":false,"solution":true,"checksum":"20a84a3e2de53db3d900bb010e34075e","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":144},{"cell_type":"code","source":["# TEST Training, validation and test RMSE (2c)\nassert_true(np.allclose([rmse_train_base, rmse_val_base, rmse_test_base],\n                            [21.477279181895707, 21.22695358728026, 21.127323734084705]), 'incorrect RMSE values')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_RMSE","locked":true,"solution":false,"points":2,"checksum":"eb601ca2db0141729d0ba5635cb53c9d","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":145},{"cell_type":"markdown","source":["### Visualization 3: Predicted vs. actual\n\nWe will visualize predictions on the validation dataset. The scatter plots below visualize tuples storing i) the predicted value and ii) true label.  The first scatter plot represents the ideal situation where the predicted value exactly equals the true label, while the second plot uses the baseline predictor (i.e., `average_train_year`) for all predicted values.  Further note that the points in the scatter plots are color-coded, ranging from light yellow when the true and predicted values are equal to bright red when they drastically differ."],"metadata":{}},{"cell_type":"code","source":["from matplotlib.colors import ListedColormap, Normalize\nfrom matplotlib.cm import get_cmap\ncmap = get_cmap('YlOrRd')\nnorm = Normalize()\n\ndef squared_error(lp):\n    \"\"\"Calculates the squared error for a single prediction.\"\"\"\n    label, prediction = lp\n    return float((label - prediction)**2)\n\nactual = np.asarray(parsed_val_data_df\n                    .select('label')\n                    .collect())\nerror = np.asarray(parsed_val_data_df\n                   .rdd\n                   .map(lambda lp: (lp.label, lp.label))\n                   .map(lambda lp: squared_error(lp))\n                   .collect())\nclrs = cmap(np.asarray(norm(error)))[:,0:3]\n\nfig, ax = prepare_plot(np.arange(0, 100, 20), np.arange(0, 100, 20))\nplt.scatter(actual, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=0.5)\nax.set_xlabel('Predicted'), ax.set_ylabel('Actual')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":147},{"cell_type":"code","source":["def squared_error(lp):\n    \"\"\"Calculates the squared error for a single prediction.\"\"\"\n    label, prediction = lp\n    return float((label - prediction)**2)\n\npredictions = np.asarray(parsed_val_data_df\n                         .rdd\n                         .map(lambda lp: average_train_year)\n                         .collect())\nerror = np.asarray(parsed_val_data_df\n                   .rdd\n                   .map(lambda lp: (lp.label, average_train_year))\n                   .map(lambda lp: squared_error(lp))\n                   .collect())\nnorm = Normalize()\nclrs = cmap(np.asarray(norm(error)))[:,0:3]\n\nfig, ax = prepare_plot(np.arange(53.0, 55.0, 0.5), np.arange(0, 100, 20))\nax.set_xlim(53, 55)\nplt.scatter(predictions, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=0.3)\nax.set_xlabel('Predicted'), ax.set_ylabel('Actual')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":148},{"cell_type":"markdown","source":["## Part 3: Train (via gradient descent) and evaluate a linear regression model"],"metadata":{}},{"cell_type":"markdown","source":["### (3a) Gradient summand\n\nNow let's see if we can do better via linear regression, training a model via gradient descent (we'll omit the intercept for now). Recall that the gradient descent update for linear regression is:\n\\\\[ \\scriptsize \\mathbf{w}_{i+1} = \\mathbf{w}_i - \\alpha_i \\sum_j (\\mathbf{w}_i^\\top\\mathbf{x}_j  - y_j) \\mathbf{x}_j \\,.\\\\]\nwhere \\\\( \\scriptsize i \\\\) is the iteration number of the gradient descent algorithm, and \\\\( \\scriptsize j \\\\) identifies the observation.\n\nFirst, implement a function that computes the summand for this update, i.e., the summand equals \\\\( \\scriptsize (\\mathbf{w}^\\top \\mathbf{x} - y) \\mathbf{x} \\, ,\\\\) and test out this function on two examples.  Use the `DenseVector` [dot](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.linalg.DenseVector.dot) method."],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.linalg import DenseVector"],"metadata":{},"outputs":[],"execution_count":151},{"cell_type":"code","source":["def gradient_summand(weights, lp):\n    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\n\n    Note:\n        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n        within this function.  For example, they both implement the `dot` method.\n\n    Args:\n        weights (DenseVector): An array of model weights (betas).\n        lp (LabeledPoint): The `LabeledPoint` for a single observation.\n\n    Returns:\n        DenseVector: An array of values the same length as `weights`.  The gradient summand.\n    \"\"\"\n    \n    return (weights.dot(lp.features)-lp.label)*lp.features\n\nexample_w = DenseVector([1, 1, 1])\nexample_lp = LabeledPoint(2.0, [3, 1, 4])\nsummand_one = gradient_summand(example_w, example_lp)\nprint (summand_one)\n\nexample_w = DenseVector([.24, 1.2, -1.4])\nexample_lp = LabeledPoint(3.0, [-1.4, 4.2, 2.1])\nsummand_two = gradient_summand(example_w, example_lp)\nprint (summand_two)\n"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"gradientSummand","locked":false,"solution":true,"checksum":"5894952ae939293f40c3974cdb189dd8","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":152},{"cell_type":"code","source":["# TEST Gradient summand (3a)\nassert_true(np.allclose(summand_one, [18., 6., 24.]), 'incorrect value for summand_one')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_gradientSummand","locked":true,"solution":false,"points":2,"checksum":"17c28727d606f84834ea326ce7477399","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":153},{"cell_type":"markdown","source":["### (3b) Use weights to make predictions\n\nNext, implement a `get_labeled_predictions` function that takes in weights and an observation's `LabeledPoint` and returns a _(prediction, label)_ tuple.  Note that we can predict by computing the dot product between weights and an observation's features."],"metadata":{}},{"cell_type":"code","source":["def get_labeled_prediction(weights, observation):\n    \"\"\"Calculates predictions and returns a (prediction, label) tuple.\n\n    Note:\n        The labels should remain unchanged as we'll use this information to calculate prediction\n        error later.\n\n    Args:\n        weights (np.ndarray): An array with one weight for each features in `trainData`.\n        observation (LabeledPoint): A `LabeledPoint` that contain the correct label and the\n            features for the data point.\n\n    Returns:\n        tuple: A (prediction, label) tuple. Convert the return type of the label and prediction to a float.\n    \"\"\"\n    return (float(weights.dot(observation.features)), float(observation.label))#<FILL IN>\n\nweights = np.array([1.0, 1.5])\nprediction_example = sc.parallelize([LabeledPoint(2, np.array([1.0, .5])),\n                                     LabeledPoint(1.5, np.array([.5, .5]))])\npreds_and_labels_example = prediction_example.map(lambda lp: get_labeled_prediction(weights, lp))\nprint (preds_and_labels_example.collect())"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"getPrediction","locked":false,"solution":true,"checksum":"093ae884a8a88abe9cea87235f2b4a82","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":155},{"cell_type":"code","source":["# TEST Use weights to make predictions (3b)\nassert_true(isinstance(preds_and_labels_example.first()[0], float), 'prediction must be a float')\nassert_equal(preds_and_labels_example.collect(), [(1.75, 2.0), (1.25, 1.5)],\n                  'incorrect definition for getLabeledPredictions')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_getPrediction","locked":true,"solution":false,"points":2,"checksum":"cd4c3fe7287b4ed7d3045db02a603fed","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":156},{"cell_type":"markdown","source":["### (3c) Gradient descent\n\nNext, implement a gradient descent function for linear regression and test out this function on an example."],"metadata":{}},{"cell_type":"code","source":["def linreg_gradient_descent(train_data, num_iters):\n    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n\n    Note:\n        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n        within this function.  For example, they both implement the `dot` method.\n\n    Args:\n        train_data (RDD of LabeledPoint): The labeled data for use in training the model.\n        num_iters (int): The number of iterations of gradient descent to perform.\n\n    Returns:\n        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n            final weights (one weight per feature) for the model, and training errors will contain\n            an error (RMSE) for each iteration of the algorithm.\n    \"\"\"\n    # The length of the training data\n    n = train_data.count()\n    # The number of features in the training data\n    d = len(train_data.first().features)\n    w = np.zeros(d) \n    alpha = 1.0\n    # We will compute and store the training error after each iteration\n    error_train = np.zeros(num_iters)\n    for i in range(num_iters):\n        # Use get_labeled_prediction from (3b) with trainData to obtain an RDD of (label, prediction)\n        # tuples.  Note that the weights all equal 0 for the first iteration, so the predictions will\n        # have large errors to start.\n        preds_and_labels_train = train_data.map(lambda lp: get_labeled_prediction(w, lp))\n    \n        preds_and_labels_train_df = sqlContext.createDataFrame(preds_and_labels_train, [\"prediction\", \"label\"])\n        error_train[i] = calc_RMSE(preds_and_labels_train_df)\n\n        # Calculate the `gradient`.  Make use of the `gradient_summand` function you wrote in (3a).\n        # Note that `gradient` should be a `DenseVector` of length `d`.\n        gradient = train_data.map(lambda x: gradient_summand(w, x)).sum()\n\n        # Update the weights\n        alpha_i = alpha / (n * np.sqrt(i+1))\n        w -= alpha_i*gradient\n    return w, error_train\n\n# create a toy dataset with n = 10, d = 3, and then run 5 iterations of gradient descent\n# note: the resulting model will not be useful; the goal here is to verify that\n# linreg_gradient_descent is working properly\nexample_n = 10\nexample_d = 3\nexample_data = (sc\n                 .parallelize(parsed_train_data_df.take(example_n))\n                 .map(lambda lp: LabeledPoint(lp.label, lp.features[0:example_d])))\nprint (example_data.take(2))\nexample_num_iters = 5\nexample_weights, example_error_train = linreg_gradient_descent(example_data, example_num_iters)\nprint (example_weights)\n"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"gradientDescent","locked":false,"solution":true,"checksum":"ae2bc9e183087963a1cee15ece6b7d02","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":158},{"cell_type":"code","source":["# TEST Gradient descent (3c)\n\nexpected_output = [ 3.6706757 , 28.77277769, 17.91957382]\nassert_true(np.allclose(example_weights, expected_output), 'value of example_weights is incorrect')\nexpected_error = [38.71433843 ,32.45595388, 30.22343598, 29.10818648, 28.46448674]\nassert_true(np.allclose(example_error_train, expected_error),\n                'value of exampleErrorTrain is incorrect')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_gradientDescent","locked":true,"solution":false,"points":4,"checksum":"126f9ad1b2c3180ce027b05a8bbb66c6","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":159},{"cell_type":"markdown","source":["### (3d) Train the model\n\nNow let's train a linear regression model on all of our training data and evaluate its accuracy on the validation set.  Note that the test set will not be used here.  If we evaluated the model on the test set, we would bias our final results.\n\nWe've already done much of the required work: we computed the number of features in Part (1b); we created the training and validation datasets and computed their sizes in Part (1e); and, we wrote a function to compute RMSE in Part (2b)."],"metadata":{}},{"cell_type":"code","source":["num_iters = 50\nweights_LR0, error_train_LR0 = linreg_gradient_descent(parsed_train_data_df.rdd, num_iters)\n\npreds_and_labels = (parsed_val_data_df\n                      .rdd\n                      .map(lambda lp: get_labeled_prediction(weights_LR0, lp)))\npreds_and_labels_df = sqlContext.createDataFrame(preds_and_labels, [\"prediction\", \"label\"])\nrmse_val_LR0 = calc_RMSE(preds_and_labels_df)\n\nprint ('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}'.format(rmse_val_base,\n                                                                       rmse_val_LR0))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"trainingModel","locked":false,"solution":true,"checksum":"028993fe31646db19fdec38fc85ccef2","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":161},{"cell_type":"code","source":["# TEST Train the model (3d)\nexpected_output =[22.49753756, 20.37597243, -0.27251763,  8.31521579,  5.88853769, -4.82949119,\n                   15.60187764,  3.97275651,  9.93959836,  6.06173598, 10.9900427,   3.72333163]\nassert_true(np.allclose(weights_LR0, expected_output), 'incorrect value for weights_LR0')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_trainingModel","locked":true,"solution":false,"points":2,"checksum":"a8322796f13301a10317bc2b80b38edf","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":162},{"cell_type":"markdown","source":["### Visualization 4: Training error\n\nWe will look at the log of the training error as a function of iteration. The first scatter plot visualizes the logarithm of the training error for all 50 iterations.  The second plot shows the training error itself, focusing on the final 44 iterations."],"metadata":{}},{"cell_type":"code","source":["norm = Normalize()\nclrs = cmap(np.asarray(norm(np.log(error_train_LR0))))[:,0:3]\n\nfig, ax = prepare_plot(np.arange(0, 60, 10), np.arange(2, 6, 1))\nax.set_ylim(2, 6)\nplt.scatter(range(0, num_iters), np.log(error_train_LR0), s=14**2, c=clrs, edgecolors='#888888', alpha=0.75)\nax.set_xlabel('Iteration'), ax.set_ylabel(r'$\\log_e(errorTrainLR0)$')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":164},{"cell_type":"code","source":["norm = Normalize()\nclrs = cmap(np.asarray(norm(error_train_LR0[6:])))[:,0:3]\n\nfig, ax = prepare_plot(np.arange(0, 60, 10), np.arange(17, 22, 1))\nax.set_ylim(17.8, 21.2)\nplt.scatter(range(0, num_iters-6), error_train_LR0[6:], s=14**2, c=clrs, edgecolors='#888888', alpha=0.75)\nax.set_xticklabels(map(str, range(6, 66, 10)))\nax.set_xlabel('Iteration'), ax.set_ylabel(r'Training Error')\ndisplay(fig)"],"metadata":{},"outputs":[],"execution_count":165},{"cell_type":"markdown","source":["## Part 4: Train using SparkML and perform grid search"],"metadata":{}},{"cell_type":"markdown","source":["### (4a) `LinearRegression`\nWe're already doing better than the baseline model, but let's see if we can do better by adding an intercept, using regularization, and (based on the previous visualization) training for more iterations. MLlib's [LinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionWithSGD) essentially applys smae idea that we used in Part (3b), albeit using stochastic gradient approximation and more efficiently with various additional functionality including an intercept in the model and also allowing L1 or L2 regularization.\n\nFirst use [LinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionWithSGD) to train a model with L2 regularization and with an intercept. This method returns a LinearRegressionModel. Next, use the model's weights (ex model.weights) and intercept (ex model.intercept) attributes to print out the model's parameters"],"metadata":{}},{"cell_type":"code","source":["from pyspark.mllib.regression import LinearRegressionWithSGD\n# Values to use when training the linear regression model\nnumIters = 500  # iterations\nalpha = 1.0  # step\nminiBatchFrac = 1.0  # miniBatchFraction\nreg = 1e-1  # regParam\nregType = 'l2'  # regType\nuseIntercept = True  # intercept"],"metadata":{},"outputs":[],"execution_count":168},{"cell_type":"code","source":["first_model = LinearRegressionWithSGD.train(data=parsed_train_data_df.rdd.map(lambda lp: LabeledPoint(lp.label, lp.features)), \n                                           iterations=numIters, \n                                           step=alpha, \n                                           miniBatchFraction=miniBatchFrac, \n                                           initialWeights=None, \n                                           regParam=reg, \n                                           regType=regType, \n                                           intercept=useIntercept\n                                           )\n\ncoeffs_LR1 = first_model.weights\nintercept_LR1 = first_model.intercept\nprint (coeffs_LR1, intercept_LR1)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"LR","locked":false,"solution":true,"checksum":"351df1595861ae4b2747174b6da42af4","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":169},{"cell_type":"code","source":["# TEST LinearRegression (4a)\nexpected_intercept = 13.272917647663501\nexpected_weights= [15.90811601077196,14.141747071778694,0.6361803544493579,6.1325094710269825,3.974387960587376,-2.5115097734889575,10.55122835853148,3.108716057088987,7.247819807528351,4.703102469082578,7.760457489026197,3.0765704673716043]\n\nassert_true(np.allclose(intercept_LR1, expected_intercept), 'incorrect value for intercept_LR1')\nassert_true(np.allclose(coeffs_LR1, expected_weights), 'incorrect value for weights_LR1')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_LR","locked":true,"solution":false,"points":2,"checksum":"35e68c2a0865cc3f03ac14c6d579f0ba","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":170},{"cell_type":"markdown","source":["### (4b) Predict\n\nNow use the [LinearRegressionModel.predict()](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.predict) method to make a prediction on a sample point. Pass the features from a LabeledPoint into the predict() method"],"metadata":{}},{"cell_type":"code","source":["samplePoint = parsed_train_data_df.take(1)[0]\nprint(samplePoint)\nsample_prediction = first_model.predict(samplePoint.features)\nprint (sample_prediction)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"predict_LR","locked":false,"solution":true,"checksum":"ec40d3d6f106a2101061dcd0c4a9d682","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":172},{"cell_type":"code","source":["# TEST Predict (4b)\nassert_true(np.allclose(sample_prediction, 39.82206386491484),\n                'incorrect value for sample_prediction')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_predict_LR","locked":true,"solution":false,"points":2,"checksum":"62bb4e0fc2c578ef8479ce0a124106c0","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":173},{"cell_type":"markdown","source":["### (4c) Evaluate RMSE\n\nNext evaluate the accuracy of this model on the validation set.  First, map [LinearRegressionModel.predict()](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LinearRegressionModel.predict) method on features in parsed_val_data_df to get labelsAndPreds. Create a dataframe based on that and then use the `calc_RMSE()` function from Part (2b)."],"metadata":{}},{"cell_type":"code","source":["labels_and_preds = parsed_val_data_df.rdd.map(lambda lp: (lp.label, float(first_model.predict(lp.features))))\nlabels_and_preds_df = sqlContext.createDataFrame(labels_and_preds, [\"label\", \"prediction\"])\nrmse_val_LR1 = calc_RMSE(labels_and_preds_df)\n\nprint(rmse_val_LR1)"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"evaluateRMSE","locked":false,"solution":true,"checksum":"bd292e7567de189f769761a70d1d85d6","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":175},{"cell_type":"code","source":["# TEST Evaluate RMSE (4c)\nassert_true(np.allclose(rmse_val_LR1, 19.49452264747249), 'incorrect value for rmseValLR1')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_evaluateRMSE","locked":true,"solution":false,"points":2,"checksum":"e048d67f0e1267b801ab32f921a86a1f","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":176},{"cell_type":"markdown","source":["### (4d) Grid search\n\nWe're already outperforming the baseline on the validation set by almost 2 years on average, but let's see if we can do better. Perform grid search to find a good regularization parameter.  Try `regParam` values `1e-10`, `1e-5`, and `1.0`."],"metadata":{}},{"cell_type":"code","source":["best_RMSE = rmse_val_LR1\nbest_reg_param = reg\nbest_model = first_model\n\nnum_iters = 500  # iterations\nalpha = 1.0 # step\nminiBatchFrac = 1.0\n\nfor reg in [1e-10, 1e-5, 1.0]:\n    model = LinearRegressionWithSGD.train(data=parsed_train_data_df.rdd.map(lambda lp: LabeledPoint(lp.label, lp.features)), \n                                           iterations=num_iters, \n                                           step=alpha, \n                                           miniBatchFraction=miniBatchFrac, \n                                           initialWeights=None, \n                                           regParam=reg, \n                                           regType=regType, \n                                           intercept=useIntercept\n                                           )\n    \n    labels_and_preds = parsed_val_data_df.rdd.map(lambda lp: (lp.label, float(model.predict(lp.features))))\n    \n    labels_and_preds_df = sqlContext.createDataFrame(labels_and_preds, [\"label\", \"prediction\"])\n    rmse_val_grid = calc_RMSE(labels_and_preds_df)\n    print (rmse_val_grid)\n\n    if rmse_val_grid < best_RMSE:\n        best_RMSE = rmse_val_grid\n        best_reg_param = reg\n        best_model = model\n\nrmse_val_LR_grid = best_RMSE\n\nprint (('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n' +\n       '\\tLRGrid = {3:.3f}').format(rmse_val_base, rmse_val_LR0, rmse_val_LR1, rmse_val_LR_grid))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"bestRMSE","locked":false,"solution":true,"checksum":"6bb086c3b4a1b8e5d30ee7001642874a","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":178},{"cell_type":"code","source":["# TEST Grid search (4d)\nassert_true(np.allclose(17.19165402610348, rmse_val_LR_grid), 'incorrect value for rmseValLRGrid')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_bestRMSE","locked":true,"solution":false,"points":3,"checksum":"60e0e5d84bd1a6de8cf8cda448496993","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":179},{"cell_type":"markdown","source":["### Visualization 5: Best model's predictions\n\nNext, we create a visualization similar to 'Visualization 3: Predicted vs. actual' from Part 2 using the predictions from the best model from Part (4d) on the validation dataset.  Specifically, we create a color-coded scatter plot visualizing tuples storing i) the predicted value from this model and ii) true label."],"metadata":{}},{"cell_type":"code","source":["predictions = np.asarray(parsed_val_data_df.rdd\n                         .map(lambda lp: best_model.predict(lp.features))\n                         .collect())\nactual = np.asarray(parsed_val_data_df.rdd\n                    .map(lambda lp: lp.label)\n                    .collect())\nerror = np.asarray(parsed_val_data_df.rdd\n                   .map(lambda lp: (lp.label, best_model.predict(lp.features)))\n                   .map( lambda lp: squared_error(lp))\n                   .collect())\n\nnorm = Normalize()\nclrs = cmap(np.asarray(norm(error)))[:,0:3]\n\nfig, ax = prepare_plot(np.arange(0, 120, 20), np.arange(0, 120, 20))\nax.set_xlim(15, 82), ax.set_ylim(-5, 105)\nplt.scatter(predictions, actual, s=14**2, c=clrs, edgecolors='#888888', alpha=0.75, linewidths=.5)\nax.set_xlabel('Predicted'), ax.set_ylabel(r'Actual')\ndisplay(fig) \npass"],"metadata":{},"outputs":[],"execution_count":181},{"cell_type":"markdown","source":["### Visualization 6: Hyperparameter heat map\n\nNext, we perform a visualization of hyperparameter search using a larger set of hyperparameters (with precomputed results).  Specifically, we create a heat map where the brighter colors correspond to lower RMSE values.  The first plot has a large area with brighter colors.  In order to differentiate within the bright region, we generate a second plot corresponding to the hyperparameters found within that region."],"metadata":{}},{"cell_type":"code","source":["from matplotlib.colors import LinearSegmentedColormap\n\n# Saved parameters and results, to save the time required to run 36 models\nnumItersParams = [10, 50, 100, 250, 500, 1000]\nregParams = [1e-8, 1e-6, 1e-4, 1e-2, 1e-1, 1]\nrmseVal = np.array([[  20.36769649,   20.36770128,   20.36818057,   20.41795354,  21.09778437,  301.54258421],\n                    [  19.04948826,   19.0495    ,   19.05067418,   19.16517726,  19.97967727,   23.80077467],\n                    [  18.40149024,   18.40150998,   18.40348326,   18.59457491,  19.82155716,   23.80077467],\n                    [  17.5609346 ,   17.56096749,   17.56425511,   17.88442127,  19.71577117,   23.80077467],\n                    [  17.0171705 ,   17.01721288,   17.02145207,   17.44510574,  19.69124734,   23.80077467],\n                    [  16.58074813,   16.58079874,   16.58586512,   17.11466904,  19.6860931 ,   23.80077467]])\n\nnumRows, numCols = len(numItersParams), len(regParams)\nrmseVal = np.array(rmseVal)\nrmseVal.shape = (numRows, numCols)\n\nfig, ax = prepare_plot(np.arange(0, numCols, 1), np.arange(0, numRows, 1), figsize=(8, 7), hideLabels=True,\n                      gridWidth=0.)\nax.set_xticklabels(regParams), ax.set_yticklabels(numItersParams)\nax.set_xlabel('Regularization Parameter'), ax.set_ylabel('Number of Iterations')\n\ncolors = LinearSegmentedColormap.from_list('blue', ['#0022ff', '#000055'], gamma=.2)\nimage = plt.imshow(rmseVal,interpolation='nearest', aspect='auto',\n                    cmap = colors)\ndisplay(fig) "],"metadata":{},"outputs":[],"execution_count":183},{"cell_type":"code","source":["# Zoom into the bottom left\nnumItersParamsZoom, regParamsZoom = numItersParams[-3:], regParams[:4]\nrmseValZoom = rmseVal[-3:, :4]\n\nnumRows, numCols = len(numItersParamsZoom), len(regParamsZoom)\n\nfig, ax = prepare_plot(np.arange(0, numCols, 1), np.arange(0, numRows, 1), figsize=(8, 7), hideLabels=True,\n                      gridWidth=0.)\nax.set_xticklabels(regParamsZoom), ax.set_yticklabels(numItersParamsZoom)\nax.set_xlabel('Regularization Parameter'), ax.set_ylabel('Number of Iterations')\n\ncolors = LinearSegmentedColormap.from_list('blue', ['#0022ff', '#000055'], gamma=.2)\nimage = plt.imshow(rmseValZoom,interpolation='nearest', aspect='auto',\n                    cmap = colors)\ndisplay(fig) \npass"],"metadata":{},"outputs":[],"execution_count":184},{"cell_type":"markdown","source":["## Part 5: Add interactions between features"],"metadata":{}},{"cell_type":"markdown","source":["### (5a) Add 2-way interactions\n\nSo far, we've used the features as they were provided.  Now, we will add features that capture the two-way interactions between our existing features.  Write a function `two_way_interactions` that takes in a `LabeledPoint` and generates a new `LabeledPoint` that contains the old features and the two-way interactions between them.\n\n> Note:\n> * A dataset with three features would have nine ( \\\\( \\scriptsize 3^2 \\\\) ) two-way interactions.\n> * You might want to use [itertools.product](https://docs.python.org/2/library/itertools.html#itertools.product) to generate tuples for each of the possible 2-way interactions.\n> * Remember that you can combine two `DenseVector` or `ndarray` objects using [np.hstack](http://docs.scipy.org/doc/numpy/reference/generated/numpy.hstack.html#numpy.hstack)."],"metadata":{}},{"cell_type":"code","source":["import itertools\n\ndef two_way_interactions(lp):\n    \"\"\"Creates a new `LabeledPoint` that includes two-way interactions.\n\n    Note:\n        For features [x, y] the two-way interactions would be [x^2, x*y, y*x, y^2] and these\n        would be appended to the original [x, y] feature list.\n\n    Args:\n        lp (LabeledPoint): The label and features for this observation.\n\n    Returns:\n        LabeledPoint: The new `LabeledPoint` should have the same label as `lp`.  Its features\n            should include the features from `lp` followed by the two-way interaction features.\n\n    \"\"\"\n    \n    arr = [e[0]*e[1] for e in itertools.product(lp.features, lp.features)]\n    new_features = np.append(lp.features, arr)\n    return  LabeledPoint(lp.label, new_features)\n\n# Transform the existing train, validation, and test sets to include two-way interactions.\n# Remember to convert them back to DataFrames at the end.\ntrain_data_interact_df = parsed_train_data_df.rdd.map(lambda lp: two_way_interactions(lp)).toDF()\nval_data_interact_df = parsed_val_data_df.rdd.map(lambda lp: two_way_interactions(lp)).toDF()\ntest_data_interact_df = parsed_test_data_df.rdd.map(lambda lp: two_way_interactions(lp)).toDF()"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"twoWayInteractions","locked":false,"solution":true,"checksum":"a32680343ea54932f9e156ebf58e9b80","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":187},{"cell_type":"code","source":["# TEST Add two-way interactions (5a)\ntwo_way_example = two_way_interactions(LabeledPoint(0.0, [2, 3]))\n\nassert_true(np.allclose(sorted(two_way_example.features),\n                            sorted([2.0, 3.0, 4.0, 6.0, 6.0, 9.0])),\n                'incorrect features generatedBy two_way_interactions')\n\ntwo_way_point = two_way_interactions(LabeledPoint(1.0, [1, 2, 3]))\n\nassert_true(np.allclose(sorted(two_way_point.features),\n                            sorted([1.0,2.0,3.0,1.0,2.0,3.0,2.0,4.0,6.0,3.0,6.0,9.0])),\n                'incorrect features generated by twoWayInteractions')\n\n\nassert_equal(two_way_point.label, 1.0, 'incorrect label generated by two_way_interactions')\n"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_twoWayInteractions","locked":true,"solution":false,"points":3,"checksum":"4b551751df5d7f709104c054dff3ecea","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":188},{"cell_type":"markdown","source":["### (5b) Build interaction model\n\nNow, let's build the new model.  We've done this several times now.  To implement this for the new features, we need to change a few variable names.\n\n > Note:\n > * Remember that we should build our model from the training data and evaluate it on the validation data.\n > * You should re-run your hyperparameter search after changing features, as using the best hyperparameters from your prior model will not necessary lead to the best model.\n > * For this exercise, we have already preset the hyperparameters to reasonable values."],"metadata":{}},{"cell_type":"code","source":["numIters = 500\nalpha = 1.0\nminiBatchFrac = 1.0\nreg = 1e-10\n\nmodel_interact = LinearRegressionWithSGD.train(data=train_data_interact_df.rdd.map(lambda lp: LabeledPoint(lp.label, lp.features)), \n                                           iterations=num_iters, \n                                           step=alpha, \n                                           miniBatchFraction=miniBatchFrac, \n                                           initialWeights=None, \n                                           regParam=reg,\n                                           regType=regType, \n                                           intercept=useIntercept)\n\nlabels_and_preds_interact = val_data_interact_df.rdd.map(lambda lp: (lp.label, float(model_interact.predict(lp.features))))\nlabels_and_preds_interact_df = sqlContext.createDataFrame(labels_and_preds_interact, [\"label\", \"prediction\"])\nrmse_val_interact = calc_RMSE(labels_and_preds_interact_df)\n\nprint (('Validation RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLR0 = {1:.3f}\\n\\tLR1 = {2:.3f}\\n\\tLRGrid = ' +\n       '{3:.3f}\\n\\tLRInteract = {4:.3f}').format(rmse_val_base, rmse_val_LR0, rmse_val_LR1,\n                                                 rmse_val_LR_grid, rmse_val_interact))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"buildInteractionModel","locked":false,"solution":true,"checksum":"fa34834c5acc39a63a9ad0ebcbddd6f3","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":190},{"cell_type":"code","source":["# TEST Build interaction model (5b)\nassert_true(np.allclose(rmse_val_interact, 16.197614458395268), 'incorrect value for rmse_val_interact')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_buildInteractionModel","locked":true,"solution":false,"points":2,"checksum":"5aa1a614a85db3a63902470768f9da0b","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":191},{"cell_type":"markdown","source":["### (5c) Evaluate interaction model on test data\n\nOur next step is to evaluate the new model on the test dataset.  Note that we haven't used the test set to evaluate any of our models.  Because of this, our evaluation provides us with an unbiased estimate for how our model will perform on new data.  If we had changed our model based on viewing its performance on the test set, our estimate of RMSE would likely be overly optimistic.\n\nWe'll also print the RMSE for both the baseline model and our new model.  With this information, we can see how much better our model performs than the baseline model."],"metadata":{}},{"cell_type":"code","source":["labels_and_preds_interact_test = test_data_interact_df.rdd.map(lambda lp: (lp.label, float(model_interact.predict(lp.features))))\nlabels_and_preds_interact_test_df = sqlContext.createDataFrame(labels_and_preds_interact_test, [\"label\", \"prediction\"])\nrmse_test_interact = calc_RMSE(labels_and_preds_interact_test_df)\n\nprint (('Test RMSE:\\n\\tBaseline = {0:.3f}\\n\\tLRInteract = {1:.3f}'\n       .format(rmse_test_base, rmse_test_interact)))"],"metadata":{"deletable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"evaluateInteractionModel","locked":false,"solution":true,"checksum":"6848d2fc667c54cd56e0a0469f3f1f2a","grade":false,"cell_type":"code"}},"outputs":[],"execution_count":193},{"cell_type":"code","source":["# TEST Evaluate interaction model on test data (5c)\nassert_true(np.allclose(rmse_test_interact, 16.605307423246618),\n                'incorrect value for rmse_test_interact')"],"metadata":{"deletable":false,"editable":false,"nbgrader":{"task":false,"schema_version":3,"grade_id":"test_evaluateInteractionModel","locked":true,"solution":false,"points":2,"checksum":"acdee9a16e3e7ebb29671a51916c4114","grade":true,"cell_type":"code"}},"outputs":[],"execution_count":194}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"hw2 draft 2.0","notebookId":2270421742722486},"nbformat":4,"nbformat_minor":0}
